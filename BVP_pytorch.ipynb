{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import and parameters\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "# load data\n",
    "from tools import data_loader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('deps/sparse_rrt')\n",
    "sys.path.append('.')\n",
    "from sparse_rrt import _sst_module\n",
    "\n",
    "seen_N = 1\n",
    "seen_NP = 1\n",
    "seen_s = 0\n",
    "seen_sp = 800\n",
    "data_folder = './data/acrobot_obs/'\n",
    "test_data = data_loader.load_test_dataset(seen_N, seen_NP,\n",
    "                      data_folder, True, seen_s, seen_sp)\n",
    "obc, obs, paths, sgs, path_lengths, controls, costs = test_data\n",
    "obs_width = 6.0\n",
    "step_sz = 0.02\n",
    "num_steps = 10\n",
    "dt = 0.02\n",
    "system = _sst_module.PSOPTAcrobot()\n",
    "cpp_propagator = _sst_module.SystemPropagator()\n",
    "propagate_dynamics = lambda x, u, t: cpp_propagator.propagate(system, x, u, t)\n",
    "bound = [3.141592653589793, 3.141592653589793, 6.0, 6.0]\n",
    "def wrap_angle(x, system):\n",
    "    circular = system.is_circular_topology()\n",
    "    res = np.array(x)\n",
    "    for i in range(len(x)):\n",
    "        if circular[i]:\n",
    "            # use our previously saved version\n",
    "            res[i] = x[i] - np.floor(x[i] / (2*np.pi))*(2*np.pi)\n",
    "            if res[i] > np.pi:\n",
    "                res[i] = res[i] - 2*np.pi\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define dynamics\n",
    "def dynamics(state, control):\n",
    "    STATE_THETA_1, STATE_THETA_2, STATE_V_1, STATE_V_2 = 0, 1, 2, 3\n",
    "    MIN_V_1, MAX_V_1 = -6., 6.\n",
    "    MIN_V_2, MAX_V_2 = -6., 6.\n",
    "    MIN_TORQUE, MAX_TORQUE = -4., 4.\n",
    "\n",
    "    MIN_ANGLE, MAX_ANGLE = -np.pi, np.pi\n",
    "\n",
    "    LENGTH = 20.\n",
    "    m = 1.0\n",
    "    lc = 0.5\n",
    "    lc2 = 0.25\n",
    "    l2 = 1.\n",
    "    I1 = 0.2\n",
    "    I2 = 1.0\n",
    "    l = 1.0\n",
    "    g = 9.81\n",
    "\n",
    "    theta2 = state[STATE_THETA_2]\n",
    "    theta1 = state[STATE_THETA_1] - np.pi/2\n",
    "    theta1dot = state[STATE_V_1]\n",
    "    theta2dot = state[STATE_V_2]\n",
    "    _tau = control[0]\n",
    "    m = m\n",
    "    l2 = l2\n",
    "    lc2 = lc2\n",
    "    l = l\n",
    "    lc = lc\n",
    "    I1 = I1\n",
    "    I2 = I2\n",
    "\n",
    "    d11 = m * lc2 + m * (l2 + lc2 + 2 * l * lc * torch.cos(theta2)) + I1 + I2\n",
    "    d22 = m * lc2 + I2\n",
    "    d12 = m * (lc2 + l * lc * torch.cos(theta2)) + I2\n",
    "    d21 = d12\n",
    "\n",
    "    c1 = -m * l * lc * theta2dot * theta2dot * torch.sin(theta2) - (2 * m * l * lc * theta1dot * theta2dot * torch.sin(theta2))\n",
    "    c2 = m * l * lc * theta1dot * theta1dot * torch.sin(theta2)\n",
    "    g1 = (m * lc + m * l) * g * torch.cos(theta1) + (m * lc * g * torch.cos(theta1 + theta2))\n",
    "    g2 = m * lc * g * torch.cos(theta1 + theta2)\n",
    "\n",
    "    #deriv[STATE_THETA_1] = theta1dot\n",
    "    #deriv[STATE_THETA_2] = theta2dot\n",
    "\n",
    "    u2 = _tau - 1 * .1 * theta2dot\n",
    "    u1 = -1 * .1 * theta1dot\n",
    "    theta1dot_dot = (d22 * (u1 - c1 - g1) - d12 * (u2 - c2 - g2)) / (d11 * d22 - d12 * d21)\n",
    "    theta2dot_dot = (d11 * (u2 - c2 - g2) - d21 * (u1 - c1 - g1)) / (d11 * d22 - d12 * d21)\n",
    "    #deriv[STATE_V_1] = theta1dot_dot\n",
    "    #deriv[STATE_V_2] = theta2dot_dot\n",
    "    deriv = torch.stack([theta1dot, theta2dot, theta1dot_dot, theta2dot_dot], 0)\n",
    "    return deriv\n",
    "\n",
    "def torch_enforce_state_bounds(state):\n",
    "    STATE_THETA_1, STATE_THETA_2, STATE_V_1, STATE_V_2 = 0, 1, 2, 3\n",
    "    MIN_V_1, MAX_V_1 = -6., 6.\n",
    "    MIN_V_2, MAX_V_2 = -6., 6.\n",
    "    MIN_TORQUE, MAX_TORQUE = -4., 4.\n",
    "\n",
    "    MIN_ANGLE, MAX_ANGLE = -np.pi, np.pi\n",
    "    state=torch.empty(4, requires_grad=False)\n",
    "    if state[0] < -np.pi:\n",
    "        state[0] += 2*np.pi\n",
    "    elif state[0] > np.pi:\n",
    "        state[0] -= 2 * np.pi\n",
    "    if state[1] < -np.pi:\n",
    "        state[1] += 2*np.pi\n",
    "    elif state[1] > np.pi:\n",
    "        state[1] -= 2 * np.pi\n",
    "\n",
    "    if state[2] < MIN_V_1:\n",
    "        state[2] = MIN_V_1\n",
    "    if state[2] > MAX_V_1:\n",
    "        state[2] = MAX_V_1\n",
    "    if state[3] < MIN_V_2:\n",
    "        state[3] = MIN_V_2\n",
    "    if state[3] > MAX_V_2:\n",
    "        state[3] = MAX_V_2\n",
    "    \n",
    "    return state\n",
    "\n",
    "def torch_enforce_control_bounds(control):\n",
    "    MIN_TORQUE, MAX_TORQUE = -4., 4.\n",
    "    control = torch.empty(1, requires_grad=False)\n",
    "    if control[0] < MIN_TORQUE:\n",
    "        control[0] = MIN_TORQUE\n",
    "    if control[0] > MAX_TORQUE:\n",
    "        control[0] = MAX_TORQUE\n",
    "    return control\n",
    "\n",
    "    \n",
    "def enforce_state_bounds(state):\n",
    "    STATE_THETA_1, STATE_THETA_2, STATE_V_1, STATE_V_2 = 0, 1, 2, 3\n",
    "    MIN_V_1, MAX_V_1 = -6., 6.\n",
    "    MIN_V_2, MAX_V_2 = -6., 6.\n",
    "    MIN_TORQUE, MAX_TORQUE = -4., 4.\n",
    "\n",
    "    MIN_ANGLE, MAX_ANGLE = -np.pi, np.pi\n",
    "    \n",
    "    if state[0] < -np.pi:\n",
    "        state[0] += 2*np.pi\n",
    "    elif state[0] > np.pi:\n",
    "        state[0] -= 2 * np.pi\n",
    "    if state[1] < -np.pi:\n",
    "        state[1] += 2*np.pi\n",
    "    elif state[1] > np.pi:\n",
    "        state[1] -= 2 * np.pi\n",
    "\n",
    "    if state[2] < MIN_V_1:\n",
    "        state[2] = MIN_V_1\n",
    "    if state[2] > MAX_V_1:\n",
    "        state[2] = MAX_V_1\n",
    "    if state[3] < MIN_V_2:\n",
    "        state[3] = MIN_V_2\n",
    "    if state[3] > MAX_V_2:\n",
    "        state[3] = MAX_V_2\n",
    "    \n",
    "    return state\n",
    "\n",
    "def enforce_control_bounds(control):\n",
    "    MIN_TORQUE, MAX_TORQUE = -4., 4.\n",
    "    if control[0] < MIN_TORQUE:\n",
    "        control[0] = MIN_TORQUE\n",
    "    if control[0] > MAX_TORQUE:\n",
    "        control[0] = MAX_TORQUE\n",
    "    return control\n",
    "\n",
    "    \n",
    "\n",
    "def IsInCollision(x, obc, obc_width=6.):\n",
    "    STATE_THETA_1, STATE_THETA_2, STATE_V_1, STATE_V_2 = 0, 1, 2, 3\n",
    "    MIN_V_1, MAX_V_1 = -6., 6.\n",
    "    MIN_V_2, MAX_V_2 = -6., 6.\n",
    "    MIN_TORQUE, MAX_TORQUE = -4., 4.\n",
    "\n",
    "    MIN_ANGLE, MAX_ANGLE = -np.pi, np.pi\n",
    "\n",
    "    LENGTH = 20.\n",
    "    m = 1.0\n",
    "    lc = 0.5\n",
    "    lc2 = 0.25\n",
    "    l2 = 1.\n",
    "    I1 = 0.2\n",
    "    I2 = 1.0\n",
    "    l = 1.0\n",
    "    g = 9.81\n",
    "    pole_x0 = 0.\n",
    "    pole_y0 = 0.\n",
    "    pole_x1 = LENGTH * np.cos(x[STATE_THETA_1] - np.pi / 2)\n",
    "    pole_y1 = LENGTH * np.sin(x[STATE_THETA_1] - np.pi / 2)\n",
    "    pole_x2 = pole_x1 + LENGTH * np.cos(x[STATE_THETA_1] + x[STATE_THETA_2] - np.pi / 2)\n",
    "    pole_y2 = pole_y1 + LENGTH * np.sin(x[STATE_THETA_1] + x[STATE_THETA_2] - np.pi / 2)\n",
    "    for i in range(len(obc)):\n",
    "        for j in range(0, 8, 2):\n",
    "            x1 = obc[i][j]\n",
    "            y1 = obc[i][j+1]\n",
    "            x2 = obc[i][(j+2) % 8]\n",
    "            y2 = obc[i][(j+3) % 8]\n",
    "            if line_line_cc(pole_x0, pole_y0, pole_x1, pole_y1, x1, y1, x2, y2):\n",
    "                return True\n",
    "            if line_line_cc(pole_x1, pole_y1, pole_x2, pole_y2, x1, y1, x2, y2):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def line_line_cc(x1, y1, x2, y2, x3, y3, x4, y4):\n",
    "    uA = ((x4-x3)*(y1-y3) - (y4-y3)*(x1-x3)) / ((y4-y3)*(x2-x1) - (x4-x3)*(y2-y1))\n",
    "    uB = ((x2-x1)*(y1-y3) - (y2-y1)*(x1-x3)) / ((y4-y3)*(x2-x1) - (x4-x3)*(y2-y1))\n",
    "    if uA >= 0. and uA <= 1. and uB >= 0. and uB <= 1.:\n",
    "        # intersection\n",
    "        return True\n",
    "    # collision free\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the BVP solver\n",
    "def dynamic_loss(x0, u0, x1, num_dt, dt):\n",
    "    bound = torch.tensor([3.141592653589793, 3.141592653589793, 6.0, 6.0])\n",
    "    x_target = x0\n",
    "    for i in range(num_dt):\n",
    "        x_target = dynamics(x_target, u0) * dt + x_target\n",
    "    \n",
    "    #x_target_norm = x_target / bound\n",
    "    #x1_norm = x1 / bound  # normalize everything to -1 ~ 1\n",
    "    #return torch.mean((x_target_norm - x1_norm) ** 2) / 4\n",
    "    res = x_target - x1\n",
    "    circular = system.is_circular_topology()\n",
    "\n",
    "    for i in range(len(x_target)):\n",
    "        if circular[i]:\n",
    "            if res[i] > np.pi:\n",
    "                res[i] = res[i] - 2*np.pi\n",
    "            if res[i] < -np.pi:\n",
    "                res[i] = res[i] + 2*np.pi\n",
    "    #res = (x_target - x1) ** 2\n",
    "    #res = res ** 2\n",
    "    res = torch.abs(res)\n",
    "    res = torch.mean(res)\n",
    "    #if res <= 1.:\n",
    "    #    res = res * (1 / res.detach().data.item())\n",
    "    return res\n",
    "\n",
    "def bvp_solve_naive(x_init, u_init, dt, num_steps):\n",
    "    # there are num_steps - 1 intermediate nodes\n",
    "    #x0 = torch.tensor(start)\n",
    "    x0 = torch.from_numpy(x_init[0]).cuda()\n",
    "    #xG = torch.tensor(goal)\n",
    "    xG = torch.from_numpy(x_init[-1]).cuda()\n",
    "    xs = torch.from_numpy(x_init)\n",
    "    us = torch.from_numpy(u_init)\n",
    "\n",
    "    x0 = Variable(x0, requires_grad=False).cuda().detach()\n",
    "    xG = Variable(xG, requires_grad=False).cuda().detach()\n",
    "    \n",
    "    xs = Variable(xs, requires_grad=True).cuda().detach()\n",
    "    xs.requires_grad = True\n",
    "    #xs = [Variable(xs[i], requires_grad=True) for i in range(len(xs))]\n",
    "    us = Variable(us, requires_grad=True).cuda().detach()\n",
    "    us.requires_grad = True\n",
    "    #us = [Variable(us[i], requires_grad=True) for i in range(len(us))]\n",
    "    #opt_algo = torch.optim.Adagrad\n",
    "    opt_algo = torch.optim.Adam\n",
    "    \n",
    "    opt = opt_algo([xs, us], lr=1e-1)\n",
    "    \n",
    "    \n",
    "    max_iter = 1000\n",
    "    for opt_iter in range(max_iter):        \n",
    "        # calcualte the start loss\n",
    "        l0 = dynamic_loss(x0, us[0], xs[1], 1, dt)\n",
    "        l_sum = l0\n",
    "        lts = []\n",
    "        for t in range(1, len(xs)-2):\n",
    "            lt = dynamic_loss(xs[t], us[t], xs[t+1], 1, dt)\n",
    "            l_sum += lt\n",
    "            lts.append(lt.cpu().item())\n",
    "        # calculate the goal loss\n",
    "        lT = dynamic_loss(xs[-2], us[-1], xG, 1, dt)\n",
    "        l_sum += lT\n",
    "        #l_sum *= 10000\n",
    "        # optimize by SGD\n",
    "        opt.zero_grad()\n",
    "        xs.retain_grad()\n",
    "        us.retain_grad()\n",
    "        l_sum.backward()\n",
    "        opt.step()\n",
    "        # enforce bounds on states & controls\n",
    "        with torch.no_grad():\n",
    "            for t in range(1,len(xs)-1):\n",
    "                enforce_state_bounds(xs[t])\n",
    "\n",
    "            for t in range(len(us)):\n",
    "                enforce_control_bounds(us[t])\n",
    "        #print('####################')\n",
    "        #print('iteration %d: ' % (opt_iter))\n",
    "        #print('loss: ')\n",
    "        #print('l_0: ', l0.item())\n",
    "        #print('lts: ', lts)\n",
    "        #print('l_T: ', lT.item())\n",
    "        \n",
    "        #for t in range(1, len(xs)-1):\n",
    "        #    #print('l_%d: %f' % (t, lts[t-1].item()))\n",
    "        #print('l_T: ', lT.item())\n",
    "        if opt_iter == 0 or opt_iter == max_iter-1:\n",
    "            print('l_sum: ', l_sum.cpu().item())\n",
    "            print('l_0: ', l0.cpu().item())\n",
    "            print('lts: ', lts)\n",
    "            print('l_T: ', lT.cpu().item())\n",
    "        #print('####################')\n",
    "        #print('vars:')\n",
    "        #print('xs: ')\n",
    "        #print(xs.detach().data.numpy())\n",
    "        #print('us: ')\n",
    "        #print(us.detach().data.numpy())\n",
    "    \n",
    "    x0_res = np.array([x0.cpu().data.numpy()])\n",
    "    xs_res = xs.cpu().detach().data.numpy()[1:-1]\n",
    "    xT_res = np.array([xG.cpu().data.numpy()])\n",
    "    us_res = us.cpu().detach().data.numpy()\n",
    "    \n",
    "    #print(x0_res.shape)\n",
    "    #print(xs_res.shape)\n",
    "    #print(xT_res.shape)\n",
    "    x_res = np.concatenate([x0_res, xs_res, xT_res], 0)\n",
    "    u_res = us_res\n",
    "    return x_res, u_res\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bvp_solve(x_init, u_init, dt, num_steps):\n",
    "    # there are num_steps - 1 intermediate nodes\n",
    "    #x0 = torch.tensor(start)\n",
    "    x0 = torch.from_numpy(x_init[0])\n",
    "    #xG = torch.tensor(goal)\n",
    "    xG = torch.from_numpy(x_init[-1])\n",
    "    # random initialize xs\n",
    "    #xs = torch.linspace(x0, xG, num_steps-1)\n",
    "    #xs = []\n",
    "    #for i in range(len(x0)):\n",
    "    #    xs_i = torch.linspace(x0[i].item(), xG[i].item(), num_steps-1)\n",
    "    #    xs_i = torch.normal(mean=xs_i, std=0.5)\n",
    "    #    xs.append(xs_i)\n",
    "    #xs = torch.from_numpy(x_init)\n",
    "    #xs = torch.stack(xs, 0).T\n",
    "    #xs = torch.zeros([num_steps-1, len(start)])  # to optimize\n",
    "    #us = torch.zeros([num_steps, 1]).uniform_(-4., 4.)  # to optimize\n",
    "    xs = [torch.from_numpy(x_init[i]) for i in range(len(x_init))]\n",
    "    #us = torch.from_numpy(u_init)\n",
    "    us = [torch.from_numpy(u_init[i]) for i in range(len(u_init))]\n",
    "    x0 = Variable(x0, requires_grad=False)\n",
    "    xG = Variable(xG, requires_grad=False)\n",
    "    \n",
    "    #xs = Variable(xs, requires_grad=True)\n",
    "    xs = [Variable(xs[i], requires_grad=True) for i in range(len(xs))]\n",
    "    #us = Variable(us, requires_grad=True)\n",
    "    us = [Variable(us[i], requires_grad=True) for i in range(len(us))]\n",
    "    #opt_algo = torch.optim.Adagrad\n",
    "    opt_algo = torch.optim.SGD\n",
    "    \n",
    "    opt0 = opt_algo([us[0]], lr=1e-1)\n",
    "    opts = [opt_algo([xs[i], us[i]], lr=1e-1) for i in range(1, len(xs)-2)]\n",
    "    optT = opt_algo([xs[-2], us[-1]], lr=1e-1)\n",
    "    #opt = opt_algo([xs, us], lr=1e-1)\n",
    "    max_iter = 100\n",
    "    for opt_iter in range(max_iter):\n",
    "        l_sum = 0.\n",
    "        lT = dynamic_loss(xs[-2], us[-1], xG, 1, dt)\n",
    "        optT.zero_grad()\n",
    "        lT.backward()\n",
    "        optT.step()\n",
    "        l_sum += lT\n",
    "        for t in range(len(xs)-3, 0, -1):\n",
    "            lt = dynamic_loss(xs[t], us[t], xs[t+1], 1, dt)\n",
    "            opts[t-1].zero_grad()\n",
    "            lt.backward()\n",
    "            opts[t-1].step()\n",
    "            l_sum += lt\n",
    "            \n",
    "        # calcualte the start loss\n",
    "        l0 = dynamic_loss(x0, us[0], xs[1], 1, dt)\n",
    "        opt0.zero_grad()\n",
    "        l0.backward()\n",
    "        opt0.step()\n",
    "        l_sum += l0\n",
    "        with torch.no_grad():\n",
    "            for t in range(1,len(xs)-1):\n",
    "                #xs[t] = enforce_state_bounds(xs[t])\n",
    "                enforce_state_bounds(xs[t])\n",
    "                #W.data.copy(new_value.data)\n",
    "                #xs[t].data.copy(enforce_state_bounds(xs[t]))\n",
    "                #xs[t] = xs[t] * 0. + torch_enforce_state_bounds(xs[t])\n",
    "                #print(xs[t])\n",
    "            for t in range(len(us)):\n",
    "                enforce_control_bounds(us[t])\n",
    "                #us[t] = enforce_control_bounds(us[t])\n",
    "                #us[t].data.copy(enforce_control_bounds(us[t]))\n",
    "                #us[t] = us[t] * 0. + torch_enforce_control_bounds(us[t])\n",
    "                #print(us[t])\n",
    "        #print('####################')\n",
    "        #print('iteration %d: ' % (opt_iter))\n",
    "        #print('loss: ')\n",
    "        #print('l_0: ', l0.item())\n",
    "        #for t in range(1, len(xs)-1):\n",
    "        #    #print('l_%d: %f' % (t, lts[t-1].item()))\n",
    "        #print('l_T: ', lT.item())\n",
    "        if opt_iter == 0 or opt_iter == max_iter-1:\n",
    "            print('l_sum: ', l_sum.item())\n",
    "        #print('####################')\n",
    "        #print('vars:')\n",
    "        #print('xs: ')\n",
    "        #print(xs.detach().data.numpy())\n",
    "        #print('us: ')\n",
    "        #print(us.detach().data.numpy())\n",
    "    \n",
    "    x0_res = np.array([x0.data.numpy()])\n",
    "    xs_res = xs[1:-1]\n",
    "    xs_res = np.array([xs[i].detach().data.numpy() for i in range(len(xs_res))])\n",
    "    #xs_res = xs.detach().data.numpy()[1:-1]\n",
    "    xT_res = np.array([xG.data.numpy()])\n",
    "    #us_res = us.detach().data.numpy()\n",
    "    us_res = np.array([us[i].detach().data.numpy() for i in range(len(us))])\n",
    "    #print(x0_res.shape)\n",
    "    #print(xs_res.shape)\n",
    "    #print(xT_res.shape)\n",
    "    x_res = np.concatenate([x0_res, xs_res, xT_res], 0)\n",
    "    u_res = us_res\n",
    "    return x_res, u_res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bvp_solve(x_init, u_init, dt, num_steps):\n",
    "    # there are num_steps - 1 intermediate nodes\n",
    "    #x0 = torch.tensor(start)\n",
    "    x0 = torch.from_numpy(x_init[0])\n",
    "    #xG = torch.tensor(goal)\n",
    "    xG = torch.from_numpy(x_init[-1])\n",
    "    xs = torch.from_numpy(x_init)\n",
    "    us = torch.from_numpy(u_init)\n",
    "    x0 = Variable(x0, requires_grad=False)\n",
    "    xG = Variable(xG, requires_grad=False)\n",
    "    \n",
    "    #xs = Variable(xs, requires_grad=True)\n",
    "    \n",
    "    #us = Variable(us, requires_grad=True)\n",
    "    opt_algo = torch.optim.Adam\n",
    "    circular = system.is_circular_topology()\n",
    "    max_iter = 50\n",
    "    lr = 1e-1\n",
    "    for opt_iter in range(max_iter):\n",
    "        #*** project from Set B to Set A\n",
    "        # by propagating using the u\n",
    "        uA = us.clone().detach()\n",
    "        uA.requires_grad = True\n",
    "        #print(uA)\n",
    "        opt_ba_project = opt_algo([uA], lr=lr)\n",
    "        print('projection from B to A:')\n",
    "        for ba_project_i in range(10):\n",
    "            #print('iteration %d:' % (ba_project_i))\n",
    "            xA = [x0]\n",
    "            x = x0\n",
    "            for i in range(len(us)):\n",
    "                x = dt * dynamics(x, uA[i]) + x\n",
    "                xA.append(x)\n",
    "            xA = torch.stack(xA, 0)\n",
    "            l_sum = 0.\n",
    "\n",
    "            for i in range(len(xA)):\n",
    "                dif = xA[i] - xs[i]\n",
    "                dif_new = torch.empty(4)\n",
    "                for j in range(4):\n",
    "                    if circular[j]:\n",
    "                        if dif[j] >= np.pi:\n",
    "\n",
    "                            dif_new[j] = dif[j] - 2*np.pi\n",
    "                        elif dif[j] <= -np.pi:\n",
    "\n",
    "                            dif_new[j] = dif[j] + 2*np.pi\n",
    "                        else:\n",
    "                            dif_new[j] = dif[j]\n",
    "                    else:\n",
    "                        dif_new[j] = dif[j]\n",
    "\n",
    "                l_sum += torch.sum(dif_new ** 2)\n",
    "            \n",
    "            l_sum += torch.sum((uA - us) ** 2)\n",
    "            l_sum = l_sum / len(xA)\n",
    "            opt_ba_project.zero_grad()\n",
    "            l_sum.backward()\n",
    "            #uA.retain_grad()\n",
    "            uA.grad.data.clamp_(-4/lr,4/lr)\n",
    "\n",
    "            opt_ba_project.step()\n",
    "            #print(uA.grad)\n",
    "            # enforce bounds for controls\n",
    "            with torch.no_grad():\n",
    "                for t in range(len(uA)):\n",
    "                    enforce_control_bounds(uA[t])\n",
    "            if ba_project_i == 0 or ba_project_i == 99:\n",
    "                print('l_sum: ', l_sum.item())\n",
    "        # enforce bound for results of Set A\n",
    "        xA = [x0]\n",
    "        x = x0\n",
    "        for i in range(len(us)):\n",
    "            x = dt * dynamics(x, uA[i]) + x\n",
    "            enforce_state_bounds(x)\n",
    "            xA.append(x)\n",
    "        xA = torch.stack(xA)\n",
    "        #*** project from Set A to Set B (by setting xT=xG)\n",
    "        #print(\"xs:\")\n",
    "        #print(xs)\n",
    "        #print('us:')\n",
    "        #print(us)\n",
    "        #print('xA:')\n",
    "        #print(xA)\n",
    "        #print('uA:')\n",
    "        #print(uA)\n",
    "        xs = xA.detach().data\n",
    "        xs[-1] = xG\n",
    "        us = uA.clone().detach()\n",
    "\n",
    "        \n",
    "        \n",
    "    x0_res = np.array([x0.data.numpy()])\n",
    "    xs_res = xs.data.numpy()[1:-1]\n",
    "    xT_res = np.array([xG.data.numpy()])\n",
    "    us_res = us.detach().data.numpy()\n",
    "    x_res = np.concatenate([x0_res, xs_res, xT_res], 0)\n",
    "    u_res = us_res\n",
    "    return x_res, u_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dynamic_loss(x0, u0, x1, num_dt, dt):\n",
    "    bound = torch.tensor([3.141592653589793, 3.141592653589793, 6.0, 6.0])\n",
    "    x_target = x0\n",
    "    for i in range(num_dt):\n",
    "        x_target = dynamics(x_target, u0) * dt + x_target\n",
    "    \n",
    "    #x_target_norm = x_target / bound\n",
    "    #x1_norm = x1 / bound  # normalize everything to -1 ~ 1\n",
    "    #return torch.mean((x_target_norm - x1_norm) ** 2) / 4\n",
    "    res = x_target - x1\n",
    "    circular = system.is_circular_topology()\n",
    "\n",
    "    for i in range(len(x_target)):\n",
    "        if circular[i]:\n",
    "            if res[i] > np.pi:\n",
    "                res[i] = res[i] - 2*np.pi\n",
    "            if res[i] < -np.pi:\n",
    "                res[i] = res[i] + 2*np.pi\n",
    "    #res = (x_target - x1) ** 2\n",
    "    #res = res ** 2\n",
    "\n",
    "    res = torch.abs(res)\n",
    "    cond = res < 1\n",
    "    l = torch.where(cond, 0.5 * res ** 2, res)\n",
    "    #l = torch.mean(l, dim=0)  # sum alone the batch dimension, now the dimension is the same as input dimension\n",
    "\n",
    "    res = torch.mean(res)\n",
    "    #if res <= 1.:\n",
    "    #    res = res * (1 / res.detach().data.item())\n",
    "    return res\n",
    "\n",
    "def bvp_solve_naive(x_init, u_init, dt, num_steps):\n",
    "    # there are num_steps - 1 intermediate nodes\n",
    "    x0 = torch.from_numpy(x_init[0])\n",
    "    xG = torch.from_numpy(x_init[-1])\n",
    "    xs = [torch.from_numpy(x_init[i]) for i in range(len(x_init))]\n",
    "    us = [torch.from_numpy(u_init[i]) for i in range(len(u_init))]\n",
    "    x0 = Variable(x0, requires_grad=False)\n",
    "    xG = Variable(xG, requires_grad=False)\n",
    "    \n",
    "    xs = [Variable(xs[i], requires_grad=True) for i in range(len(xs))]\n",
    "    us = [Variable(us[i], requires_grad=True) for i in range(len(us))]\n",
    "    \n",
    "    #us = [Variable(us[i], requires_grad=True) for i in range(len(us))]\n",
    "    #opt_algo = torch.optim.Adagrad\n",
    "    opt_algo = torch.optim.Adam\n",
    "    \n",
    "    #opt = opt_algo([xs, us], lr=1e-1)\n",
    "    \n",
    "    \n",
    "    max_iter = 20\n",
    "    for opt_iter in range(max_iter):        \n",
    "        # forward pass\n",
    "        with torch.no_grad():\n",
    "            #for i in range(len(us)-2, -1, -1):\n",
    "            for i in range(len(us)-1):\n",
    "                xs[i+1] = dt*dynamics(xs[i], us[i]) + xs[i]\n",
    "                with torch.no_grad():\n",
    "                        enforce_state_bounds(xs[i+1])\n",
    "        # backward pass using loss of endpoint to update u and x\n",
    "        loss_sum = 0.\n",
    "        for i in range(len(xs)-2, 0, -1):\n",
    "            # using next state as a fixed target to optimize current state and control\n",
    "\n",
    "            xs[i].requires_grad = True\n",
    "            us[i].requires_grad = True\n",
    "            opt_i = opt_algo([xs[i], us[i]], lr=1e-2)\n",
    "            xs[i+1].detach()\n",
    "            for j in range(5):\n",
    "                def closure():\n",
    "                    loss = dynamic_loss(xs[i], us[i], xs[i+1], 1, dt)\n",
    "                    #print('xs[i]: ', xs[i])\n",
    "                    #print('us[i]: ', us[i])\n",
    "                    #print('xs[i+1]: ', xs[i+1])\n",
    "                    #print('dynamic loss: ', loss)\n",
    "                    opt_i.zero_grad()\n",
    "                    loss.backward()\n",
    "\n",
    "                    return loss\n",
    "                opt_i.step(closure)\n",
    "            with torch.no_grad():\n",
    "                loss = dynamic_loss(xs[i], us[i], xs[i+1], 1, dt)\n",
    "                loss_sum += loss.item()\n",
    "                \n",
    "                enforce_state_bounds(xs[i])\n",
    "                enforce_control_bounds(us[i])\n",
    "            #loss_sum += loss.item()\n",
    "        # handle first state\n",
    "        us[0].requires_grad = True\n",
    "        opt_i = opt_algo([x0, us[0]], lr=1e-1)\n",
    "        xs[1].detach()\n",
    "        for j in range(10):\n",
    "            def closure():\n",
    "                loss = dynamic_loss(x0, us[0], xs[1], 1, dt)\n",
    "                opt_i.zero_grad()\n",
    "                loss.backward()\n",
    "                return loss\n",
    "            opt_i.step(closure)\n",
    "        loss = dynamic_loss(x0, us[0], xs[1], 1, dt)\n",
    "        loss_sum += loss.item()\n",
    "        print('iteration ', opt_iter)\n",
    "        print('loss_sum: ', loss_sum)\n",
    "    x0_res = np.array([x0.data.numpy()])\n",
    "    xs_res = xs[1:-1]\n",
    "    xs_res = np.array([xs[i].detach().data.numpy() for i in range(len(xs_res))])\n",
    "    xT_res = np.array([xG.data.numpy()])\n",
    "    us_res = np.array([us[i].detach().data.numpy() for i in range(len(us))]) \n",
    "    \n",
    "    #print(x0_res.shape)\n",
    "    #print(xs_res.shape)\n",
    "    #print(xT_res.shape)\n",
    "    x_res = np.concatenate([x0_res, xs_res, xT_res], 0)\n",
    "    u_res = us_res\n",
    "    return x_res, u_res\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dynamic_loss(x0, u0, x1, num_dt, dt):\n",
    "    bound = torch.tensor([3.141592653589793, 3.141592653589793, 6.0, 6.0])\n",
    "    x_target = x0\n",
    "    for i in range(num_dt):\n",
    "        x_target = dynamics(x_target, u0) * dt + x_target\n",
    "    \n",
    "    #x_target_norm = x_target / bound\n",
    "    #x1_norm = x1 / bound  # normalize everything to -1 ~ 1\n",
    "    #return torch.mean((x_target_norm - x1_norm) ** 2) / 4\n",
    "    res = x_target - x1\n",
    "    circular = system.is_circular_topology()\n",
    "\n",
    "    for i in range(len(x_target)):\n",
    "        if circular[i]:\n",
    "            if res[i] > np.pi:\n",
    "                res[i] = res[i] - 2*np.pi\n",
    "            if res[i] < -np.pi:\n",
    "                res[i] = res[i] + 2*np.pi\n",
    "    #res = (x_target - x1) ** 2\n",
    "    #res = res ** 2\n",
    "\n",
    "    res = torch.abs(res)\n",
    "    #cond = res < 1\n",
    "    #l = torch.where(cond, 0.5 * res ** 2, res)\n",
    "    #l = torch.mean(l, dim=0)  # sum alone the batch dimension, now the dimension is the same as input dimension\n",
    "    \n",
    "    res = res ** 2\n",
    "    res = torch.mean(res)\n",
    "    #if res <= 1.:\n",
    "    #    res = res * (1 / res.detach().data.item())\n",
    "    return res\n",
    "\n",
    "def bvp_solve_naive(x_init, u_init, dt, num_steps):\n",
    "    # there are num_steps - 1 intermediate nodes\n",
    "    x0 = torch.from_numpy(x_init[0])\n",
    "    xG = torch.from_numpy(x_init[-1])\n",
    "    xs = [torch.from_numpy(x_init[i]) for i in range(len(x_init))]\n",
    "    us = [torch.from_numpy(u_init[i]) for i in range(len(u_init))]\n",
    "    x0 = Variable(x0, requires_grad=False)\n",
    "    xG = Variable(xG, requires_grad=False)\n",
    "    \n",
    "    xs = [Variable(xs[i], requires_grad=False) for i in range(len(xs))]\n",
    "    us = [Variable(us[i], requires_grad=True) for i in range(len(us))]\n",
    "    \n",
    "    #us = [Variable(us[i], requires_grad=True) for i in range(len(us))]\n",
    "    #opt_algo = torch.optim.Adagrad\n",
    "    opt_algo = torch.optim.LBFGS\n",
    "    \n",
    "    opt = opt_algo(us, lr=1e-1, max_iter=2, history_size=10, tolerance_change=1e-6)#, lr=1e-1)\n",
    "    \n",
    "    \n",
    "    max_iter = 40\n",
    "    for opt_iter in range(max_iter):        \n",
    "        # forward pass\n",
    "        print('iteration ', opt_iter)\n",
    "        #for i in range(len(us)-2, -1, -1):\n",
    "        def closure():\n",
    "            for i in range(len(us)-1):\n",
    "                xs[i+1] = dt*dynamics(xs[i], us[i]) + xs[i]\n",
    "\n",
    "            loss = dynamic_loss(xs[-2], us[-1], xs[-1], 1, dt)\n",
    "            print('loss: ', loss.item())\n",
    "\n",
    "            opt.zero_grad()\n",
    "            if loss.item() > 1e-3:\n",
    "                loss.backward()\n",
    "            return loss\n",
    "        opt.step(closure)\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(us)):\n",
    "                enforce_state_bounds(xs[i])\n",
    "                enforce_control_bounds(us[i])\n",
    "\n",
    "    x0_res = np.array([x0.data.numpy()])\n",
    "    xs_res = xs[1:-1]\n",
    "    xs_res = np.array([xs[i].detach().data.numpy() for i in range(len(xs_res))])\n",
    "    xT_res = np.array([xG.data.numpy()])\n",
    "    us_res = np.array([us[i].detach().data.numpy() for i in range(len(us))]) \n",
    "    \n",
    "    #print(x0_res.shape)\n",
    "    #print(xs_res.shape)\n",
    "    #print(xT_res.shape)\n",
    "    x_res = np.concatenate([x0_res, xs_res, xT_res], 0)\n",
    "    u_res = us_res\n",
    "    return x_res, u_res\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feasible points\n",
      "[[-3.14159265 -3.14159265  0.          0.        ]\n",
      " [-3.14159265 -3.04159265  0.          0.        ]\n",
      " [-3.14159265 -2.94159265  0.          0.        ]\n",
      " ...\n",
      " [ 2.95840735  2.75840735  0.          0.        ]\n",
      " [ 2.95840735  2.85840735  0.          0.        ]\n",
      " [ 2.95840735  2.95840735  0.          0.        ]]\n",
      "infeasible points\n",
      "[[-3.14159265  1.05840735  0.          0.        ]\n",
      " [-3.14159265  1.15840735  0.          0.        ]\n",
      " [-3.14159265  1.25840735  0.          0.        ]\n",
      " ...\n",
      " [ 2.95840735  1.45840735  0.          0.        ]\n",
      " [ 2.95840735  1.55840735  0.          0.        ]\n",
      " [ 2.95840735  1.65840735  0.          0.        ]]\n",
      "start_i: 0/276\n",
      "start:\n",
      "[0. 0. 0. 0.]\n",
      "goal:\n",
      "[ 0.07386929 -0.15716603  0.77629971 -1.66701264]\n",
      "iteration  0\n",
      "loss:  1.6388724204220373\n",
      "loss:  1.6350156955495363\n",
      "iteration  1\n",
      "loss:  1.6350156955495363\n",
      "loss:  1.322293388948235\n",
      "iteration  2\n",
      "loss:  1.322293388948235\n",
      "loss:  1.0698127199843812\n",
      "iteration  3\n",
      "loss:  1.0698127199843812\n",
      "loss:  0.8662924602613162\n",
      "iteration  4\n",
      "loss:  0.8662924602613162\n",
      "loss:  0.7019360022362664\n",
      "iteration  5\n",
      "loss:  0.7019360022362664\n",
      "loss:  0.5690211431007686\n",
      "iteration  6\n",
      "loss:  0.5690211431007686\n",
      "loss:  0.4614208299059812\n",
      "iteration  7\n",
      "loss:  0.4665888037504733\n",
      "loss:  0.37396519140372236\n",
      "iteration  8\n",
      "loss:  0.3805039718955876\n",
      "loss:  0.30315203008458913\n",
      "iteration  9\n",
      "loss:  0.30857144441006973\n",
      "loss:  0.2458849114998809\n",
      "iteration  10\n",
      "loss:  0.2502484303788211\n",
      "loss:  0.19943742832421257\n",
      "iteration  11\n",
      "loss:  0.20295056104563008\n",
      "loss:  0.1617608620651251\n",
      "iteration  12\n",
      "loss:  0.16458931029011134\n",
      "loss:  0.13119710239092058\n",
      "iteration  13\n",
      "loss:  0.13347437910896642\n",
      "loss:  0.10640296668344204\n",
      "iteration  14\n",
      "loss:  0.10823657978888161\n",
      "loss:  0.08628965399751456\n",
      "iteration  15\n",
      "loss:  0.08776613609552263\n",
      "loss:  0.06997415052049588\n",
      "iteration  16\n",
      "loss:  0.07346466695337714\n",
      "loss:  0.05621806141322595\n",
      "iteration  17\n",
      "loss:  0.060798166301638173\n",
      "loss:  0.045169532361141615\n",
      "iteration  18\n",
      "loss:  0.049291681189724504\n",
      "loss:  0.03662386398091386\n",
      "iteration  19\n",
      "loss:  0.03996107923144407\n",
      "loss:  0.029693270407210655\n",
      "iteration  20\n",
      "loss:  0.032395007337012595\n",
      "loss:  0.024073103946665394\n",
      "iteration  21\n",
      "loss:  0.02626022272432164\n",
      "loss:  0.01951594368731716\n",
      "iteration  22\n",
      "loss:  0.021648976314499142\n",
      "loss:  0.015664074811722946\n",
      "iteration  23\n",
      "loss:  0.017717044265199197\n",
      "loss:  0.012614025584450489\n",
      "iteration  24\n",
      "loss:  0.014357644132455057\n",
      "loss:  0.010224196775927742\n",
      "iteration  25\n",
      "loss:  0.011634879947375408\n",
      "loss:  0.008286910936426202\n",
      "iteration  26\n",
      "loss:  0.009428187354988184\n",
      "loss:  0.0067165780808808185\n",
      "iteration  27\n",
      "loss:  0.007639838714221866\n",
      "loss:  0.00544375648374696\n",
      "iteration  28\n",
      "loss:  0.006190590936592674\n",
      "loss:  0.004412127672299886\n",
      "iteration  29\n",
      "loss:  0.0050161944631831705\n",
      "loss:  0.0035760208639853616\n",
      "iteration  30\n",
      "loss:  0.004064560436471704\n",
      "loss:  0.0028984033096619184\n",
      "iteration  31\n",
      "loss:  0.0032934621400406874\n",
      "loss:  0.0023492491182815\n",
      "iteration  32\n",
      "loss:  0.0026686701593156953\n",
      "loss:  0.0019042154636410705\n",
      "iteration  33\n",
      "loss:  0.002229487871484302\n",
      "loss:  0.0014990221808402178\n",
      "iteration  34\n",
      "loss:  0.0018315899053760545\n",
      "loss:  0.001197055677894567\n",
      "iteration  35\n",
      "loss:  0.0015235440553306736\n",
      "loss:  0.0009342342425491012\n",
      "iteration  36\n",
      "loss:  0.001257735724784311\n",
      "loss:  0.0007335193822660321\n",
      "iteration  37\n",
      "loss:  0.0010194750156126227\n",
      "loss:  0.000595155812667743\n",
      "iteration  38\n",
      "loss:  0.0008264422487103957\n",
      "iteration  39\n",
      "loss:  0.0008264422487103957\n",
      "start_i: 10/276\n",
      "start:\n",
      "[ 0.07386929 -0.15716603  0.77629971 -1.66701264]\n",
      "goal:\n",
      "[ 0.2679138  -0.58540799  1.02294754 -2.373213  ]\n",
      "iteration  0\n",
      "loss:  1.2359675488016157\n",
      "loss:  1.2332452636739883\n",
      "iteration  1\n",
      "loss:  1.2332452636739883\n",
      "loss:  1.0091163891609405\n",
      "iteration  2\n",
      "loss:  1.0091163891609405\n",
      "loss:  0.8262113853742408\n",
      "iteration  3\n",
      "loss:  0.8262113853742408\n",
      "loss:  0.6760165298539561\n",
      "iteration  4\n",
      "loss:  0.6760165298539561\n",
      "loss:  0.5527557356505294\n",
      "iteration  5\n",
      "loss:  0.5558141354379247\n",
      "loss:  0.45154032866006905\n",
      "iteration  6\n",
      "loss:  0.4599660856835712\n",
      "loss:  0.3680969895796791\n",
      "iteration  7\n",
      "loss:  0.3753956815661828\n",
      "loss:  0.3003137295272825\n",
      "iteration  8\n",
      "loss:  0.3062034519219232\n",
      "loss:  0.2448633506651618\n",
      "iteration  9\n",
      "loss:  0.24961778206952334\n",
      "loss:  0.19953852214521162\n",
      "iteration  10\n",
      "loss:  0.203964603685101\n",
      "loss:  0.16244971367748806\n",
      "iteration  11\n",
      "loss:  0.16784766084319713\n",
      "loss:  0.13194658201979986\n",
      "iteration  12\n",
      "loss:  0.13652718797821747\n",
      "loss:  0.10734426141393179\n",
      "iteration  13\n",
      "loss:  0.11101470476892238\n",
      "loss:  0.08729521698505742\n",
      "iteration  14\n",
      "loss:  0.09024067700033758\n",
      "loss:  0.07096633737761968\n",
      "iteration  15\n",
      "loss:  0.07333319232833858\n",
      "loss:  0.05767416356495096\n",
      "iteration  16\n",
      "loss:  0.05957868112145407\n",
      "loss:  0.04685905868851999\n",
      "iteration  17\n",
      "loss:  0.04844647678639824\n",
      "loss:  0.03805617754117721\n",
      "iteration  18\n",
      "loss:  0.04062430435103211\n",
      "loss:  0.030516647794531088\n",
      "iteration  19\n",
      "loss:  0.032988319010436924\n",
      "loss:  0.02478036253224542\n",
      "iteration  20\n",
      "loss:  0.026784096073092387\n",
      "loss:  0.020118513419710646\n",
      "iteration  21\n",
      "loss:  0.021743390892201696\n",
      "loss:  0.016331420774212586\n",
      "iteration  22\n",
      "loss:  0.01764916283488545\n",
      "loss:  0.013255796674272199\n",
      "iteration  23\n",
      "loss:  0.014324548101706268\n",
      "loss:  0.01075860838424805\n",
      "iteration  24\n",
      "loss:  0.011625504510177662\n",
      "loss:  0.008731530982007733\n",
      "iteration  25\n",
      "loss:  0.009434780493549316\n",
      "loss:  0.007086402318730926\n",
      "iteration  26\n",
      "loss:  0.00765697677976797\n",
      "loss:  0.005751502609762784\n",
      "iteration  27\n",
      "loss:  0.0062145067314927985\n",
      "loss:  0.004668511990263011\n",
      "iteration  28\n",
      "loss:  0.005044294807919346\n",
      "loss:  0.0037900246995353727\n",
      "iteration  29\n",
      "loss:  0.004095080600316229\n",
      "loss:  0.0030775196923832587\n",
      "iteration  30\n",
      "loss:  0.0033252187798168123\n",
      "loss:  0.0024997051840593126\n",
      "iteration  31\n",
      "loss:  0.0027008853278652225\n",
      "loss:  0.002031169040974009\n",
      "iteration  32\n",
      "loss:  0.0021946157933753173\n",
      "loss:  0.0016512792381802824\n",
      "iteration  33\n",
      "loss:  0.0017841148147804216\n",
      "loss:  0.0013432885490385373\n",
      "iteration  34\n",
      "loss:  0.0014512870851238265\n",
      "loss:  0.0010936059002339565\n",
      "iteration  35\n",
      "loss:  0.0011814489450169834\n",
      "loss:  0.0008912036399350706\n",
      "iteration  36\n",
      "loss:  0.0009752485320850047\n",
      "iteration  37\n",
      "loss:  0.0009752485320850047\n",
      "iteration  38\n",
      "loss:  0.0009752485320850047\n",
      "iteration  39\n",
      "loss:  0.0009752485320850047\n",
      "start_i: 20/276\n",
      "start:\n",
      "[ 0.2679138  -0.58540799  1.02294754 -2.373213  ]\n",
      "goal:\n",
      "[ 0.40409827 -0.94872122  0.22703708 -1.08689643]\n",
      "iteration  0\n",
      "loss:  0.0626026611950522\n",
      "loss:  0.06249768337140605\n",
      "iteration  1\n",
      "loss:  0.06249768337140605\n",
      "loss:  0.050842568777485206\n",
      "iteration  2\n",
      "loss:  0.050842568777485206\n",
      "loss:  0.04136877753851521\n",
      "iteration  3\n",
      "loss:  0.04136877753851521\n",
      "loss:  0.03364690422026337\n",
      "iteration  4\n",
      "loss:  0.03364690422026337\n",
      "loss:  0.027356779163459535\n",
      "iteration  5\n",
      "loss:  0.027356779163459535\n",
      "loss:  0.022235702514890023\n",
      "iteration  6\n",
      "loss:  0.022235702514890023\n",
      "loss:  0.01806844625624489\n",
      "iteration  7\n",
      "loss:  0.01806844625624489\n",
      "loss:  0.014678865737657597\n",
      "iteration  8\n",
      "loss:  0.014678865737657597\n",
      "loss:  0.011922944432012236\n",
      "iteration  9\n",
      "loss:  0.011922944432012236\n",
      "loss:  0.00968303948635857\n",
      "iteration  10\n",
      "loss:  0.00968303948635857\n",
      "loss:  0.0078631303010485\n",
      "iteration  11\n",
      "loss:  0.0078631303010485\n",
      "loss:  0.006384902492035499\n",
      "iteration  12\n",
      "loss:  0.006384902492035499\n",
      "loss:  0.005184525765679016\n",
      "iteration  13\n",
      "loss:  0.005184525765679016\n",
      "loss:  0.004210006781698027\n",
      "iteration  14\n",
      "loss:  0.0042451539847906715\n",
      "loss:  0.003417257745934936\n",
      "iteration  15\n",
      "loss:  0.0034718111017817935\n",
      "loss:  0.002773173861490713\n",
      "iteration  16\n",
      "loss:  0.0028183829639622585\n",
      "loss:  0.0022523329223434895\n",
      "iteration  17\n",
      "loss:  0.0022883390458395877\n",
      "loss:  0.0018297926063921986\n",
      "iteration  18\n",
      "loss:  0.00185842796319905\n",
      "loss:  0.001487040446652385\n",
      "iteration  19\n",
      "loss:  0.0015097748662248624\n",
      "loss:  0.0012090371703133787\n",
      "iteration  20\n",
      "loss:  0.0012270505210802622\n",
      "loss:  0.0009835692696624535\n",
      "iteration  21\n",
      "loss:  0.0009978085827452605\n",
      "iteration  22\n",
      "loss:  0.0009978085827452605\n",
      "iteration  23\n",
      "loss:  0.0009978085827452605\n",
      "iteration  24\n",
      "loss:  0.0009978085827452605\n",
      "iteration  25\n",
      "loss:  0.0009978085827452605\n",
      "iteration  26\n",
      "loss:  0.0009978085827452605\n",
      "iteration  27\n",
      "loss:  0.0009978085827452605\n",
      "iteration  28\n",
      "loss:  0.0009978085827452605\n",
      "iteration  29\n",
      "loss:  0.0009978085827452605\n",
      "iteration  30\n",
      "loss:  0.0009978085827452605\n",
      "iteration  31\n",
      "loss:  0.0009978085827452605\n",
      "iteration  32\n",
      "loss:  0.0009978085827452605\n",
      "iteration  33\n",
      "loss:  0.0009978085827452605\n",
      "iteration  34\n",
      "loss:  0.0009978085827452605\n",
      "iteration  35\n",
      "loss:  0.0009978085827452605\n",
      "iteration  36\n",
      "loss:  0.0009978085827452605\n",
      "iteration  37\n",
      "loss:  0.0009978085827452605\n",
      "iteration  38\n",
      "loss:  0.0009978085827452605\n",
      "iteration  39\n",
      "loss:  0.0009978085827452605\n",
      "start_i: 30/276\n",
      "start:\n",
      "[ 0.40409827 -0.94872122  0.22703708 -1.08689643]\n",
      "goal:\n",
      "[ 0.32203132 -0.9320282  -1.22270537  1.63831284]\n",
      "iteration  0\n",
      "loss:  0.46418749665424464\n",
      "loss:  0.46348541245543\n",
      "iteration  1\n",
      "loss:  0.46348541245543\n",
      "loss:  0.37072931926059927\n",
      "iteration  2\n",
      "loss:  0.37072931926059927\n",
      "loss:  0.29636707130858286\n",
      "iteration  3\n",
      "loss:  0.29636707130858286\n",
      "loss:  0.23726932166301404\n",
      "iteration  4\n",
      "loss:  0.23726932166301404\n",
      "loss:  0.19020657129011506\n",
      "iteration  5\n",
      "loss:  0.19020657129011506\n",
      "loss:  0.15265674490324632\n",
      "iteration  6\n",
      "loss:  0.15265674490324632\n",
      "loss:  0.12264671295391029\n",
      "iteration  7\n",
      "loss:  0.12264671295391029\n",
      "loss:  0.09862700098958861\n",
      "iteration  8\n",
      "loss:  0.10088444449080397\n",
      "loss:  0.07916260474145131\n",
      "iteration  9\n",
      "loss:  0.08120531416210638\n",
      "loss:  0.06375540664409804\n",
      "iteration  10\n",
      "loss:  0.06539455091240429\n",
      "loss:  0.05138284560387736\n",
      "iteration  11\n",
      "loss:  0.052697693789612914\n",
      "loss:  0.04143636761521799\n",
      "iteration  12\n",
      "loss:  0.042492076326572745\n",
      "loss:  0.033433787139603495\n",
      "iteration  13\n",
      "loss:  0.03428213889549647\n",
      "loss:  0.026990569693304434\n",
      "iteration  14\n",
      "loss:  0.027768605677520215\n",
      "loss:  0.02178889338227103\n",
      "iteration  15\n",
      "loss:  0.022902886681680425\n",
      "loss:  0.017504940296163337\n",
      "iteration  16\n",
      "loss:  0.018504193185249828\n",
      "loss:  0.0141515508725962\n",
      "iteration  17\n",
      "loss:  0.014956873032113936\n",
      "loss:  0.01144607063987306\n",
      "iteration  18\n",
      "loss:  0.01209542529622024\n",
      "loss:  0.009262269286161116\n",
      "iteration  19\n",
      "loss:  0.009786214436827742\n",
      "loss:  0.007498902287972493\n",
      "iteration  20\n",
      "loss:  0.007923777272124248\n",
      "loss:  0.006074167353955152\n",
      "iteration  21\n",
      "loss:  0.006592675420973445\n",
      "loss:  0.004861924128395799\n",
      "iteration  22\n",
      "loss:  0.005344989821759215\n",
      "loss:  0.003944505458252171\n",
      "iteration  23\n",
      "loss:  0.0043367117173837184\n",
      "loss:  0.0032030981560424846\n",
      "iteration  24\n",
      "loss:  0.0035218046661028976\n",
      "loss:  0.0026037576310173963\n",
      "iteration  25\n",
      "loss:  0.002863005208478162\n",
      "loss:  0.002119168252106954\n",
      "iteration  26\n",
      "loss:  0.002330274121567648\n",
      "loss:  0.0017272896842191407\n",
      "iteration  27\n",
      "loss:  0.0019511313365984133\n",
      "loss:  0.0013816255147362003\n",
      "iteration  28\n",
      "loss:  0.0015967056170620034\n",
      "loss:  0.0011275203820322445\n",
      "iteration  29\n",
      "loss:  0.0013049165339528595\n",
      "loss:  0.000924635020317519\n",
      "iteration  30\n",
      "loss:  0.0010687498661569041\n",
      "loss:  0.0007604084064546453\n",
      "iteration  31\n",
      "loss:  0.0008775612830543426\n",
      "iteration  32\n",
      "loss:  0.0008775612830543426\n",
      "iteration  33\n",
      "loss:  0.0008775612830543426\n",
      "iteration  34\n",
      "loss:  0.0008775612830543426\n",
      "iteration  35\n",
      "loss:  0.0008775612830543426\n",
      "iteration  36\n",
      "loss:  0.0008775612830543426\n",
      "iteration  37\n",
      "loss:  0.0008775612830543426\n",
      "iteration  38\n",
      "loss:  0.0008775612830543426\n",
      "iteration  39\n",
      "loss:  0.0008775612830543426\n",
      "start_i: 40/276\n",
      "start:\n",
      "[ 0.32203132 -0.9320282  -1.22270537  1.63831284]\n",
      "goal:\n",
      "[-0.03879072 -0.35826642 -2.42275806  4.27116233]\n",
      "iteration  0\n",
      "loss:  0.5171825699873408\n",
      "loss:  0.5161132346927122\n",
      "iteration  1\n",
      "loss:  0.5161132346927122\n",
      "loss:  0.4151058432626004\n",
      "iteration  2\n",
      "loss:  0.4151058432626004\n",
      "loss:  0.3338021387157186\n",
      "iteration  3\n",
      "loss:  0.3338021387157186\n",
      "loss:  0.2686770110948558\n",
      "iteration  4\n",
      "loss:  0.2715724816271383\n",
      "loss:  0.21632504957731463\n",
      "iteration  5\n",
      "loss:  0.22900459529475553\n",
      "loss:  0.1715722459040438\n",
      "iteration  6\n",
      "loss:  0.192909785938588\n",
      "loss:  0.13389826705325994\n",
      "iteration  7\n",
      "loss:  0.1570032790821094\n",
      "loss:  0.1072184741611619\n",
      "iteration  8\n",
      "loss:  0.12974854001723807\n",
      "loss:  0.08431108054741449\n",
      "iteration  9\n",
      "loss:  0.1059213398192143\n",
      "loss:  0.06716180302844862\n",
      "iteration  10\n",
      "loss:  0.0855301585143429\n",
      "loss:  0.05428920635000348\n",
      "iteration  11\n",
      "loss:  0.0690909884254286\n",
      "loss:  0.04389773863023502\n",
      "iteration  12\n",
      "loss:  0.055831881272413064\n",
      "loss:  0.0355045692400678\n",
      "iteration  13\n",
      "loss:  0.04513218158048786\n",
      "loss:  0.02872321752798101\n",
      "iteration  14\n",
      "loss:  0.03649399538472764\n",
      "loss:  0.023242590359829673\n",
      "iteration  15\n",
      "loss:  0.029517421261606173\n",
      "loss:  0.01881208723865374\n",
      "iteration  16\n",
      "loss:  0.02388092475696523\n",
      "loss:  0.015229706174232421\n",
      "iteration  17\n",
      "loss:  0.01932574568505707\n",
      "loss:  0.012332526880921594\n",
      "iteration  18\n",
      "loss:  0.01564347871026145\n",
      "loss:  0.009989084501238607\n",
      "iteration  19\n",
      "loss:  0.012666159846785364\n",
      "loss:  0.008093250505017377\n",
      "iteration  20\n",
      "loss:  0.010258338505412985\n",
      "loss:  0.006559317363376652\n",
      "iteration  21\n",
      "loss:  0.008310727417156577\n",
      "loss:  0.005318046149532279\n",
      "iteration  22\n",
      "loss:  0.006836565156102062\n",
      "loss:  0.00421959404172635\n",
      "iteration  23\n",
      "loss:  0.005673715387812881\n",
      "loss:  0.0032773239672342584\n",
      "iteration  24\n",
      "loss:  0.0045981316846808815\n",
      "loss:  0.0026604270928147176\n",
      "iteration  25\n",
      "loss:  0.0037274887755045596\n",
      "loss:  0.0021608594843682983\n",
      "iteration  26\n",
      "loss:  0.003022709717159673\n",
      "loss:  0.0017562142990502965\n",
      "iteration  27\n",
      "loss:  0.0024521523156410185\n",
      "loss:  0.0014284083080078674\n",
      "iteration  28\n",
      "loss:  0.001990221835942021\n",
      "loss:  0.001162811581462825\n",
      "iteration  29\n",
      "loss:  0.001616212181632244\n",
      "loss:  0.000947586102277798\n",
      "iteration  30\n",
      "loss:  0.0013133699670141775\n",
      "loss:  0.0007731515507537803\n",
      "iteration  31\n",
      "loss:  0.0010681383805134503\n",
      "loss:  0.0006317536663978178\n",
      "iteration  32\n",
      "loss:  0.000869546131492426\n",
      "iteration  33\n",
      "loss:  0.000869546131492426\n",
      "iteration  34\n",
      "loss:  0.000869546131492426\n",
      "iteration  35\n",
      "loss:  0.000869546131492426\n",
      "iteration  36\n",
      "loss:  0.000869546131492426\n",
      "iteration  37\n",
      "loss:  0.000869546131492426\n",
      "iteration  38\n",
      "loss:  0.000869546131492426\n",
      "iteration  39\n",
      "loss:  0.000869546131492426\n",
      "start_i: 50/276\n",
      "start:\n",
      "[-0.03879072 -0.35826642 -2.42275806  4.27116233]\n",
      "goal:\n",
      "[-0.5559201   0.60972412 -2.44180717  4.90992662]\n",
      "iteration  0\n",
      "loss:  0.5201992001347536\n",
      "loss:  0.5191323999408792\n",
      "iteration  1\n",
      "loss:  0.5191323999408792\n",
      "loss:  0.4240738092727556\n",
      "iteration  2\n",
      "loss:  0.432790237801872\n",
      "loss:  0.34568896521975756\n",
      "iteration  3\n",
      "loss:  0.3540146754831695\n",
      "loss:  0.282219464086273\n",
      "iteration  4\n",
      "loss:  0.2890654118009741\n",
      "loss:  0.23027410025812503\n",
      "iteration  5\n",
      "loss:  0.23586847503381542\n",
      "loss:  0.18776766725033406\n",
      "iteration  6\n",
      "loss:  0.19728394675069377\n",
      "loss:  0.15187435994633328\n",
      "iteration  7\n",
      "loss:  0.1642364453065041\n",
      "loss:  0.12247659702270622\n",
      "iteration  8\n",
      "loss:  0.13376341552399462\n",
      "loss:  0.09970198935254873\n",
      "iteration  9\n",
      "loss:  0.10889769209692954\n",
      "loss:  0.08112227101292502\n",
      "iteration  10\n",
      "loss:  0.08861309746489038\n",
      "loss:  0.06597730508660335\n",
      "iteration  11\n",
      "loss:  0.07207625370830524\n",
      "loss:  0.053639327201349715\n",
      "iteration  12\n",
      "loss:  0.05980004174518432\n",
      "loss:  0.04303307524415961\n",
      "iteration  13\n",
      "loss:  0.05093342617574072\n",
      "loss:  0.03310036759666009\n",
      "iteration  14\n",
      "loss:  0.04138948774806779\n",
      "loss:  0.026885579532777065\n",
      "iteration  15\n",
      "loss:  0.03362728756792579\n",
      "loss:  0.02182983170712507\n",
      "iteration  16\n",
      "loss:  0.027313560123963618\n",
      "loss:  0.01772110720389748\n",
      "iteration  17\n",
      "loss:  0.0221800629704704\n",
      "loss:  0.01438311189717564\n",
      "iteration  18\n",
      "loss:  0.01800769025087025\n",
      "loss:  0.011672084105802952\n",
      "iteration  19\n",
      "loss:  0.014617614990671466\n",
      "loss:  0.009470862956085445\n",
      "iteration  20\n",
      "loss:  0.011863987611403939\n",
      "loss:  0.00768402279400152\n",
      "iteration  21\n",
      "loss:  0.009627931617472586\n",
      "loss:  0.006233882744930122\n",
      "iteration  22\n",
      "loss:  0.007812612611700074\n",
      "loss:  0.005057237897630407\n",
      "iteration  23\n",
      "loss:  0.006339193828405525\n",
      "loss:  0.004102684480330298\n",
      "iteration  24\n",
      "loss:  0.005143522662386154\n",
      "loss:  0.003328433243635583\n",
      "iteration  25\n",
      "loss:  0.004173418902384369\n",
      "loss:  0.00270052357144902\n",
      "iteration  26\n",
      "loss:  0.0033864576316286438\n",
      "loss:  0.00219136600227275\n",
      "iteration  27\n",
      "loss:  0.002748158128875887\n",
      "loss:  0.0017785536806363459\n",
      "iteration  28\n",
      "loss:  0.0022305056399504106\n",
      "loss:  0.0014438937487310875\n",
      "iteration  29\n",
      "loss:  0.0018107457273048846\n",
      "loss:  0.0011726184561249454\n",
      "iteration  30\n",
      "loss:  0.0014704015947583904\n",
      "loss:  0.0009527429920362933\n",
      "iteration  31\n",
      "loss:  0.0012075616027229768\n",
      "loss:  0.0007628785546118621\n",
      "iteration  32\n",
      "loss:  0.0009890156279921238\n",
      "iteration  33\n",
      "loss:  0.0009890156279921238\n",
      "iteration  34\n",
      "loss:  0.0009890156279921238\n",
      "iteration  35\n",
      "loss:  0.0009890156279921238\n",
      "iteration  36\n",
      "loss:  0.0009890156279921238\n",
      "iteration  37\n",
      "loss:  0.0009890156279921238\n",
      "iteration  38\n",
      "loss:  0.0009890156279921238\n",
      "iteration  39\n",
      "loss:  0.0009890156279921238\n",
      "start_i: 60/276\n",
      "start:\n",
      "[-0.5559201   0.60972412 -2.44180717  4.90992662]\n",
      "goal:\n",
      "[-0.93514565  1.46305225 -1.11722348  3.38582111]\n",
      "iteration  0\n",
      "loss:  0.20632721734749965\n",
      "loss:  0.20607278159531695\n",
      "iteration  1\n",
      "loss:  0.20607278159531695\n",
      "loss:  0.16832940422209944\n",
      "iteration  2\n",
      "loss:  0.16832940422209944\n",
      "loss:  0.1375408032903051\n",
      "iteration  3\n",
      "loss:  0.1375408032903051\n",
      "loss:  0.11228826318932827\n",
      "iteration  4\n",
      "loss:  0.11228826318932827\n",
      "loss:  0.09160214379170291\n",
      "iteration  5\n",
      "loss:  0.09160214379170291\n",
      "loss:  0.07467526357977199\n",
      "iteration  6\n",
      "loss:  0.0761334998199776\n",
      "loss:  0.060707279226113485\n",
      "iteration  7\n",
      "loss:  0.06224376016810354\n",
      "loss:  0.04941083920331384\n",
      "iteration  8\n",
      "loss:  0.05069206103593811\n",
      "loss:  0.040213717173954965\n",
      "iteration  9\n",
      "loss:  0.04126299644905334\n",
      "loss:  0.03271342777786563\n",
      "iteration  10\n",
      "loss:  0.034313901477480244\n",
      "loss:  0.02644193782402639\n",
      "iteration  11\n",
      "loss:  0.02805168492300649\n",
      "loss:  0.02146112059740802\n",
      "iteration  12\n",
      "loss:  0.022811038637107872\n",
      "loss:  0.01743997027301651\n",
      "iteration  13\n",
      "loss:  0.018832379093134306\n",
      "loss:  0.014070249612422247\n",
      "iteration  14\n",
      "loss:  0.01552724495444274\n",
      "loss:  0.011332073263352185\n",
      "iteration  15\n",
      "loss:  0.01292595563346847\n",
      "loss:  0.009022276238968771\n",
      "iteration  16\n",
      "loss:  0.010494457759324692\n",
      "loss:  0.00732332442688221\n",
      "iteration  17\n",
      "loss:  0.008596773065298297\n",
      "loss:  0.005894756343974571\n",
      "iteration  18\n",
      "loss:  0.00714224772114032\n",
      "loss:  0.004653485911156293\n",
      "iteration  19\n",
      "loss:  0.00579366907188636\n",
      "loss:  0.003775013122708701\n",
      "iteration  20\n",
      "loss:  0.0046991315801883\n",
      "loss:  0.0030618592204990336\n",
      "iteration  21\n",
      "loss:  0.0038108612190263743\n",
      "loss:  0.0024831167692134625\n",
      "iteration  22\n",
      "loss:  0.0030901271625914284\n",
      "loss:  0.002013539196353808\n",
      "iteration  23\n",
      "loss:  0.002505432574723057\n",
      "loss:  0.001632598472776192\n",
      "iteration  24\n",
      "loss:  0.002031174111984828\n",
      "loss:  0.0013236095587250544\n",
      "iteration  25\n",
      "loss:  0.0016465468034774504\n",
      "loss:  0.0010730156645991718\n",
      "iteration  26\n",
      "loss:  0.00136673170232545\n",
      "loss:  0.0008401097660618565\n",
      "iteration  27\n",
      "loss:  0.0011406992019126012\n",
      "loss:  0.0006433139448919888\n",
      "iteration  28\n",
      "loss:  0.0009246333967463454\n",
      "iteration  29\n",
      "loss:  0.0009246333967463454\n",
      "iteration  30\n",
      "loss:  0.0009246333967463454\n",
      "iteration  31\n",
      "loss:  0.0009246333967463454\n",
      "iteration  32\n",
      "loss:  0.0009246333967463454\n",
      "iteration  33\n",
      "loss:  0.0009246333967463454\n",
      "iteration  34\n",
      "loss:  0.0009246333967463454\n",
      "iteration  35\n",
      "loss:  0.0009246333967463454\n",
      "iteration  36\n",
      "loss:  0.0009246333967463454\n",
      "iteration  37\n",
      "loss:  0.0009246333967463454\n",
      "iteration  38\n",
      "loss:  0.0009246333967463454\n",
      "iteration  39\n",
      "loss:  0.0009246333967463454\n",
      "start_i: 70/276\n",
      "start:\n",
      "[-0.93514565  1.46305225 -1.11722348  3.38582111]\n",
      "goal:\n",
      "[-1.01426799  1.99896594  0.50043824  1.84161957]\n",
      "iteration  0\n",
      "loss:  0.21132749395695868\n",
      "loss:  0.21115368442341514\n",
      "iteration  1\n",
      "loss:  0.21115368442341514\n",
      "loss:  0.1721779583488821\n",
      "iteration  2\n",
      "loss:  0.17333831766028912\n",
      "loss:  0.14039992552218375\n",
      "iteration  3\n",
      "loss:  0.14387855111500691\n",
      "loss:  0.11415437168465728\n",
      "iteration  4\n",
      "loss:  0.12002872339516903\n",
      "loss:  0.09236688452762243\n",
      "iteration  5\n",
      "loss:  0.0983337988700348\n",
      "loss:  0.07507372432433092\n",
      "iteration  6\n",
      "loss:  0.08330102908705476\n",
      "loss:  0.0594623821465761\n",
      "iteration  7\n",
      "loss:  0.06861034439381002\n",
      "loss:  0.04789294395716257\n",
      "iteration  8\n",
      "loss:  0.05582231827988575\n",
      "loss:  0.038928811792463644\n",
      "iteration  9\n",
      "loss:  0.045399345365954316\n",
      "loss:  0.03163045474935708\n",
      "iteration  10\n",
      "loss:  0.03690789211825374\n",
      "loss:  0.02569230922827217\n",
      "iteration  11\n",
      "loss:  0.029994055173065616\n",
      "loss:  0.020863098262953968\n",
      "iteration  12\n",
      "loss:  0.024367679031511342\n",
      "loss:  0.016937362783833825\n",
      "iteration  13\n",
      "loss:  0.019791173762359306\n",
      "loss:  0.013747269599189515\n",
      "iteration  14\n",
      "loss:  0.016336200062577064\n",
      "loss:  0.010972983989311407\n",
      "iteration  15\n",
      "loss:  0.013486239915136592\n",
      "loss:  0.008721717192626632\n",
      "iteration  16\n",
      "loss:  0.010942329563966308\n",
      "loss:  0.0070753486616280045\n",
      "iteration  17\n",
      "loss:  0.008876966932843977\n",
      "loss:  0.005738814442940195\n",
      "iteration  18\n",
      "loss:  0.007200364739500371\n",
      "loss:  0.004654173224187303\n",
      "iteration  19\n",
      "loss:  0.005839647708033724\n",
      "loss:  0.0037741178234079174\n",
      "iteration  20\n",
      "loss:  0.0047355196969974455\n",
      "loss:  0.003060180475157664\n",
      "iteration  21\n",
      "loss:  0.003839755799049826\n",
      "loss:  0.0024810933801048176\n",
      "iteration  22\n",
      "loss:  0.0031131514055919656\n",
      "loss:  0.0020114500807352524\n",
      "iteration  23\n",
      "loss:  0.002523846788658452\n",
      "loss:  0.001630613266155496\n",
      "iteration  24\n",
      "loss:  0.002045959655307576\n",
      "loss:  0.0013218243110363363\n",
      "iteration  25\n",
      "loss:  0.0016584699488942914\n",
      "loss:  0.0010714777716192348\n",
      "iteration  26\n",
      "loss:  0.0013443109955878908\n",
      "loss:  0.0008685306451033135\n",
      "iteration  27\n",
      "loss:  0.0010896292909967106\n",
      "loss:  0.0007040216125860091\n",
      "iteration  28\n",
      "loss:  0.0008831819823924306\n",
      "iteration  29\n",
      "loss:  0.0008831819823924306\n",
      "iteration  30\n",
      "loss:  0.0008831819823924306\n",
      "iteration  31\n",
      "loss:  0.0008831819823924306\n",
      "iteration  32\n",
      "loss:  0.0008831819823924306\n",
      "iteration  33\n",
      "loss:  0.0008831819823924306\n",
      "iteration  34\n",
      "loss:  0.0008831819823924306\n",
      "iteration  35\n",
      "loss:  0.0008831819823924306\n",
      "iteration  36\n",
      "loss:  0.0008831819823924306\n",
      "iteration  37\n",
      "loss:  0.0008831819823924306\n",
      "iteration  38\n",
      "loss:  0.0008831819823924306\n",
      "iteration  39\n",
      "loss:  0.0008831819823924306\n",
      "start_i: 80/276\n",
      "start:\n",
      "[-1.01426799  1.99896594  0.50043824  1.84161957]\n",
      "goal:\n",
      "[-0.77518642  2.2351975   1.97016327  0.39438314]\n",
      "iteration  0\n",
      "loss:  0.20811375486858955\n",
      "loss:  0.20797057333943667\n",
      "iteration  1\n",
      "loss:  0.20797057333943667\n",
      "loss:  0.16934900562561997\n",
      "iteration  2\n",
      "loss:  0.16934900562561997\n",
      "loss:  0.13792507464447476\n",
      "iteration  3\n",
      "loss:  0.13933851295899297\n",
      "loss:  0.1121926756724229\n",
      "iteration  4\n",
      "loss:  0.11469618738955535\n",
      "loss:  0.09113660095000951\n",
      "iteration  5\n",
      "loss:  0.09563498213909188\n",
      "loss:  0.07359316177281135\n",
      "iteration  6\n",
      "loss:  0.07777562080963507\n",
      "loss:  0.059821075580676064\n",
      "iteration  7\n",
      "loss:  0.06322945904463055\n",
      "loss:  0.04860818291138021\n",
      "iteration  8\n",
      "loss:  0.05170228130240099\n",
      "loss:  0.03940087917464069\n",
      "iteration  9\n",
      "loss:  0.04274331460328232\n",
      "loss:  0.03172689608959978\n",
      "iteration  10\n",
      "loss:  0.034698780168975984\n",
      "loss:  0.025754476631565625\n",
      "iteration  11\n",
      "loss:  0.028162591867525264\n",
      "loss:  0.020901432144770825\n",
      "iteration  12\n",
      "loss:  0.022852929376155837\n",
      "loss:  0.01695949950229952\n",
      "iteration  13\n",
      "loss:  0.018540922592172517\n",
      "loss:  0.013758545970206145\n",
      "iteration  14\n",
      "loss:  0.015040053114029095\n",
      "loss:  0.011159956096144787\n",
      "iteration  15\n",
      "loss:  0.012198416176826828\n",
      "loss:  0.009050862279923069\n",
      "iteration  16\n",
      "loss:  0.00989236818862695\n",
      "loss:  0.0073394124730399735\n",
      "iteration  17\n",
      "loss:  0.00802132037167685\n",
      "loss:  0.00595089367721695\n",
      "iteration  18\n",
      "loss:  0.006503477220555474\n",
      "loss:  0.004824560404162645\n",
      "iteration  19\n",
      "loss:  0.005272352343775275\n",
      "loss:  0.003911042706363478\n",
      "iteration  20\n",
      "loss:  0.004273922766928335\n",
      "loss:  0.003170229902770817\n",
      "iteration  21\n",
      "loss:  0.0034643067649656367\n",
      "loss:  0.002569544175004063\n",
      "iteration  22\n",
      "loss:  0.0028078704760724077\n",
      "loss:  0.0020825332533699796\n",
      "iteration  23\n",
      "loss:  0.0022756851546141634\n",
      "loss:  0.001687724004953806\n",
      "iteration  24\n",
      "loss:  0.0018442709323956343\n",
      "loss:  0.0013676891068915666\n",
      "iteration  25\n",
      "loss:  0.0014945744484957918\n",
      "loss:  0.001108287602870818\n",
      "iteration  26\n",
      "loss:  0.001211137221595066\n",
      "loss:  0.0008980472406975012\n",
      "iteration  27\n",
      "loss:  0.0009814194768149034\n",
      "iteration  28\n",
      "loss:  0.0009814194768149034\n",
      "iteration  29\n",
      "loss:  0.0009814194768149034\n",
      "iteration  30\n",
      "loss:  0.0009814194768149034\n",
      "iteration  31\n",
      "loss:  0.0009814194768149034\n",
      "iteration  32\n",
      "loss:  0.0009814194768149034\n",
      "iteration  33\n",
      "loss:  0.0009814194768149034\n",
      "iteration  34\n",
      "loss:  0.0009814194768149034\n",
      "iteration  35\n",
      "loss:  0.0009814194768149034\n",
      "iteration  36\n",
      "loss:  0.0009814194768149034\n",
      "iteration  37\n",
      "loss:  0.0009814194768149034\n",
      "iteration  38\n",
      "loss:  0.0009814194768149034\n",
      "iteration  39\n",
      "loss:  0.0009814194768149034\n",
      "start_i: 90/276\n",
      "start:\n",
      "[-0.77518642  2.2351975   1.97016327  0.39438314]\n",
      "goal:\n",
      "[-0.29643334  2.20229201  2.72656493 -0.74535702]\n",
      "iteration  0\n",
      "loss:  0.11550856399339168\n",
      "loss:  0.11543063538055819\n",
      "iteration  1\n",
      "loss:  0.11543063538055819\n",
      "loss:  0.0938774488209681\n",
      "iteration  2\n",
      "loss:  0.0938774488209681\n",
      "loss:  0.07636176100088363\n",
      "iteration  3\n",
      "loss:  0.07636176100088363\n",
      "loss:  0.062089749256871024\n",
      "iteration  4\n",
      "loss:  0.06318830456089143\n",
      "loss:  0.0503803912094129\n",
      "iteration  5\n",
      "loss:  0.051640415209930506\n",
      "loss:  0.040909658632691054\n",
      "iteration  6\n",
      "loss:  0.04196519910084018\n",
      "loss:  0.03323369758596228\n",
      "iteration  7\n",
      "loss:  0.034849400858970064\n",
      "loss:  0.026828783768500293\n",
      "iteration  8\n",
      "loss:  0.02843408020952524\n",
      "loss:  0.021753460985631502\n",
      "iteration  9\n",
      "loss:  0.023511827097077217\n",
      "loss:  0.01751859572155104\n",
      "iteration  10\n",
      "loss:  0.019274365294316864\n",
      "loss:  0.014147818743799745\n",
      "iteration  11\n",
      "loss:  0.01564575502619799\n",
      "loss:  0.011481755584734873\n",
      "iteration  12\n",
      "loss:  0.012698283287807784\n",
      "loss:  0.009317132348889125\n",
      "iteration  13\n",
      "loss:  0.0103046171346527\n",
      "loss:  0.00756007676161451\n",
      "iteration  14\n",
      "loss:  0.008361205880190936\n",
      "loss:  0.006134117715350241\n",
      "iteration  15\n",
      "loss:  0.006783720944738351\n",
      "loss:  0.004977054968006093\n",
      "iteration  16\n",
      "loss:  0.005503526203090792\n",
      "loss:  0.0040383173929592975\n",
      "iteration  17\n",
      "loss:  0.004464785690225566\n",
      "loss:  0.0032768043557560265\n",
      "iteration  18\n",
      "loss:  0.003622096691695789\n",
      "loss:  0.002659123463649037\n",
      "iteration  19\n",
      "loss:  0.0029385549600556156\n",
      "loss:  0.0021581530914212134\n",
      "iteration  20\n",
      "loss:  0.002384174663990813\n",
      "loss:  0.0017518707144295199\n",
      "iteration  21\n",
      "loss:  0.0019345991642108452\n",
      "loss:  0.0014223985213124544\n",
      "iteration  22\n",
      "loss:  0.0015733102600448265\n",
      "loss:  0.0011539103103649744\n",
      "iteration  23\n",
      "loss:  0.0013156330045707154\n",
      "loss:  0.0009144631909385191\n",
      "iteration  24\n",
      "loss:  0.0010672305894552153\n",
      "loss:  0.0007428817248968673\n",
      "iteration  25\n",
      "loss:  0.0008658654131783518\n",
      "iteration  26\n",
      "loss:  0.0008658654131783518\n",
      "iteration  27\n",
      "loss:  0.0008658654131783518\n",
      "iteration  28\n",
      "loss:  0.0008658654131783518\n",
      "iteration  29\n",
      "loss:  0.0008658654131783518\n",
      "iteration  30\n",
      "loss:  0.0008658654131783518\n",
      "iteration  31\n",
      "loss:  0.0008658654131783518\n",
      "iteration  32\n",
      "loss:  0.0008658654131783518\n",
      "iteration  33\n",
      "loss:  0.0008658654131783518\n",
      "iteration  34\n",
      "loss:  0.0008658654131783518\n",
      "iteration  35\n",
      "loss:  0.0008658654131783518\n",
      "iteration  36\n",
      "loss:  0.0008658654131783518\n",
      "iteration  37\n",
      "loss:  0.0008658654131783518\n",
      "iteration  38\n",
      "loss:  0.0008658654131783518\n",
      "iteration  39\n",
      "loss:  0.0008658654131783518\n",
      "start_i: 100/276\n",
      "start:\n",
      "[-0.29643334  2.20229201  2.72656493 -0.74535702]\n",
      "goal:\n",
      "[ 0.23375063  2.00946975  2.34578494 -1.0228632 ]\n",
      "iteration  0\n",
      "loss:  0.1756855295802146\n",
      "loss:  0.17555530041543743\n",
      "iteration  1\n",
      "loss:  0.17555530041543743\n",
      "loss:  0.14300904345011795\n",
      "iteration  2\n",
      "loss:  0.14330612237608603\n",
      "loss:  0.11651429825616609\n",
      "iteration  3\n",
      "loss:  0.11880179013397651\n",
      "loss:  0.09466129170108603\n",
      "iteration  4\n",
      "loss:  0.09828236644549024\n",
      "loss:  0.0767536918620925\n",
      "iteration  5\n",
      "loss:  0.08103751605545996\n",
      "loss:  0.06217045052535454\n",
      "iteration  6\n",
      "loss:  0.06752992787711946\n",
      "loss:  0.04995235428944228\n",
      "iteration  7\n",
      "loss:  0.05521301662339653\n",
      "loss:  0.040476682824397495\n",
      "iteration  8\n",
      "loss:  0.0459207075176494\n",
      "loss:  0.032314033889023985\n",
      "iteration  9\n",
      "loss:  0.03730094288799983\n",
      "loss:  0.026244407388252736\n",
      "iteration  10\n",
      "loss:  0.03029168847540891\n",
      "loss:  0.021308514140930736\n",
      "iteration  11\n",
      "loss:  0.024593090728280337\n",
      "loss:  0.017296859811303224\n",
      "iteration  12\n",
      "loss:  0.01996184300272779\n",
      "loss:  0.014037500604262117\n",
      "iteration  13\n",
      "loss:  0.016758214499464713\n",
      "loss:  0.010967408873730903\n",
      "iteration  14\n",
      "loss:  0.013734396159494256\n",
      "loss:  0.008790485452060766\n",
      "iteration  15\n",
      "loss:  0.011147011646002496\n",
      "loss:  0.007130945006717646\n",
      "iteration  16\n",
      "loss:  0.009045972090962856\n",
      "loss:  0.005784191313459771\n",
      "iteration  17\n",
      "loss:  0.00734019506859269\n",
      "loss:  0.004691570677468424\n",
      "iteration  18\n",
      "loss:  0.005955640372706509\n",
      "loss:  0.003805284574275713\n",
      "iteration  19\n",
      "loss:  0.004832048705749856\n",
      "loss:  0.0030864824986269854\n",
      "iteration  20\n",
      "loss:  0.003920400890353145\n",
      "loss:  0.0025035981309066944\n",
      "iteration  21\n",
      "loss:  0.003180838753836818\n",
      "loss:  0.0020309919252028763\n",
      "iteration  22\n",
      "loss:  0.0025809658208558336\n",
      "loss:  0.0016478443725723103\n",
      "iteration  23\n",
      "loss:  0.0021424264807615966\n",
      "loss:  0.0012884513838341543\n",
      "iteration  24\n",
      "loss:  0.0017507000895644138\n",
      "loss:  0.0010320680415696123\n",
      "iteration  25\n",
      "loss:  0.001419686724600399\n",
      "loss:  0.0008380670708597301\n",
      "iteration  26\n",
      "loss:  0.0011514007672220393\n",
      "loss:  0.0006807375686090939\n",
      "iteration  27\n",
      "loss:  0.0009339670131468871\n",
      "iteration  28\n",
      "loss:  0.0009339670131468871\n",
      "iteration  29\n",
      "loss:  0.0009339670131468871\n",
      "iteration  30\n",
      "loss:  0.0009339670131468871\n",
      "iteration  31\n",
      "loss:  0.0009339670131468871\n",
      "iteration  32\n",
      "loss:  0.0009339670131468871\n",
      "iteration  33\n",
      "loss:  0.0009339670131468871\n",
      "iteration  34\n",
      "loss:  0.0009339670131468871\n",
      "iteration  35\n",
      "loss:  0.0009339670131468871\n",
      "iteration  36\n",
      "loss:  0.0009339670131468871\n",
      "iteration  37\n",
      "loss:  0.0009339670131468871\n",
      "iteration  38\n",
      "loss:  0.0009339670131468871\n",
      "iteration  39\n",
      "loss:  0.0009339670131468871\n",
      "start_i: 110/276\n",
      "start:\n",
      "[ 0.23375063  2.00946975  2.34578494 -1.0228632 ]\n",
      "goal:\n",
      "[ 0.5956463   1.86764659  1.03387666 -0.12890724]\n",
      "iteration  0\n",
      "loss:  0.37922263646353654\n",
      "loss:  0.3788993037883521\n",
      "iteration  1\n",
      "loss:  0.3788993037883521\n",
      "loss:  0.30973811005108076\n",
      "iteration  2\n",
      "loss:  0.30973811005108076\n",
      "loss:  0.2532872054534324\n",
      "iteration  3\n",
      "loss:  0.2532872054534324\n",
      "loss:  0.20693746708190563\n",
      "iteration  4\n",
      "loss:  0.20693746708190563\n",
      "loss:  0.1689349465298426\n",
      "iteration  5\n",
      "loss:  0.1689349465298426\n",
      "loss:  0.13781450652818591\n",
      "iteration  6\n",
      "loss:  0.13781450652818591\n",
      "loss:  0.11235788533482782\n",
      "iteration  7\n",
      "loss:  0.11296881797181393\n",
      "loss:  0.09154061765878728\n",
      "iteration  8\n",
      "loss:  0.09384852512285047\n",
      "loss:  0.07435409215489198\n",
      "iteration  9\n",
      "loss:  0.07796131587132413\n",
      "loss:  0.06021169713805781\n",
      "iteration  10\n",
      "loss:  0.06382026182289526\n",
      "loss:  0.04892416896576651\n",
      "iteration  11\n",
      "loss:  0.05195279661873732\n",
      "loss:  0.03980644532531397\n",
      "iteration  12\n",
      "loss:  0.04227850798716154\n",
      "loss:  0.03238101621719016\n",
      "iteration  13\n",
      "loss:  0.03458480363910482\n",
      "loss:  0.026285987896895197\n",
      "iteration  14\n",
      "loss:  0.02887459339729391\n",
      "loss:  0.021062972463205536\n",
      "iteration  15\n",
      "loss:  0.02348117904457296\n",
      "loss:  0.017127931933674033\n",
      "iteration  16\n",
      "loss:  0.019093210596659124\n",
      "loss:  0.013928004894601108\n",
      "iteration  17\n",
      "loss:  0.015612790896928519\n",
      "loss:  0.011286297775699481\n",
      "iteration  18\n",
      "loss:  0.013096998022088671\n",
      "loss:  0.008908962990366143\n",
      "iteration  19\n",
      "loss:  0.010641851866083952\n",
      "loss:  0.007245030154730812\n",
      "iteration  20\n",
      "loss:  0.008646741421119989\n",
      "loss:  0.0058928638440182875\n",
      "iteration  21\n",
      "loss:  0.007025562582950823\n",
      "loss:  0.004794488569051438\n",
      "iteration  22\n",
      "loss:  0.005708635963639934\n",
      "loss:  0.003902358244375478\n",
      "iteration  23\n",
      "loss:  0.004639154896477489\n",
      "loss:  0.0031777952134085558\n",
      "iteration  24\n",
      "loss:  0.003770837765831165\n",
      "loss:  0.0025893479017729563\n",
      "iteration  25\n",
      "loss:  0.0030659997846957786\n",
      "loss:  0.00211144954452212\n",
      "iteration  26\n",
      "loss:  0.0024939723860088678\n",
      "loss:  0.0017233231939030175\n",
      "iteration  27\n",
      "loss:  0.0020298086172389406\n",
      "loss:  0.0014080888260391347\n",
      "iteration  28\n",
      "loss:  0.0016822715735213584\n",
      "loss:  0.0011316062388408456\n",
      "iteration  29\n",
      "loss:  0.0014003132284339396\n",
      "loss:  0.0009032935709011223\n",
      "iteration  30\n",
      "loss:  0.0011425158329068604\n",
      "loss:  0.0007424219055288739\n",
      "iteration  31\n",
      "loss:  0.0009334272253901375\n",
      "iteration  32\n",
      "loss:  0.0009334272253901375\n",
      "iteration  33\n",
      "loss:  0.0009334272253901375\n",
      "iteration  34\n",
      "loss:  0.0009334272253901375\n",
      "iteration  35\n",
      "loss:  0.0009334272253901375\n",
      "iteration  36\n",
      "loss:  0.0009334272253901375\n",
      "iteration  37\n",
      "loss:  0.0009334272253901375\n",
      "iteration  38\n",
      "loss:  0.0009334272253901375\n",
      "iteration  39\n",
      "loss:  0.0009334272253901375\n",
      "start_i: 120/276\n",
      "start:\n",
      "[ 0.5956463   1.86764659  1.03387666 -0.12890724]\n",
      "goal:\n",
      "[ 0.65259933  1.9798238  -0.6514962   1.44679102]\n",
      "iteration  0\n",
      "loss:  0.2893871239504782\n",
      "loss:  0.28915300043549297\n",
      "iteration  1\n",
      "loss:  0.28915300043549297\n",
      "loss:  0.23598662819389363\n",
      "iteration  2\n",
      "loss:  0.23598662819389363\n",
      "loss:  0.19264586503040995\n",
      "iteration  3\n",
      "loss:  0.19264586503040995\n",
      "loss:  0.15713998316955352\n",
      "iteration  4\n",
      "loss:  0.15713998316955352\n",
      "loss:  0.1280871955327401\n",
      "iteration  5\n",
      "loss:  0.1280871955327401\n",
      "loss:  0.10433931905526463\n",
      "iteration  6\n",
      "loss:  0.10433931905526463\n",
      "loss:  0.0849457661966942\n",
      "iteration  7\n",
      "loss:  0.0849457661966942\n",
      "loss:  0.06912136799756044\n",
      "iteration  8\n",
      "loss:  0.06936067617554754\n",
      "loss:  0.05621240731115925\n",
      "iteration  9\n",
      "loss:  0.0571914847522513\n",
      "loss:  0.04562976102391294\n",
      "iteration  10\n",
      "loss:  0.04737213188641244\n",
      "loss:  0.03692362116928968\n",
      "iteration  11\n",
      "loss:  0.039697048858895224\n",
      "loss:  0.029585685304349282\n",
      "iteration  12\n",
      "loss:  0.03224154208258727\n",
      "loss:  0.02402752957337055\n",
      "iteration  13\n",
      "loss:  0.02618152889740162\n",
      "loss:  0.019508571775287163\n",
      "iteration  14\n",
      "loss:  0.02143521545275981\n",
      "loss:  0.015769036908269394\n",
      "iteration  15\n",
      "loss:  0.017883925852278777\n",
      "loss:  0.01253460682509162\n",
      "iteration  16\n",
      "loss:  0.01451497394595074\n",
      "loss:  0.010171737559764495\n",
      "iteration  17\n",
      "loss:  0.011778998984321758\n",
      "loss:  0.008252868665232489\n",
      "iteration  18\n",
      "loss:  0.009865369005398045\n",
      "loss:  0.006461559389225935\n",
      "iteration  19\n",
      "loss:  0.008063754669230178\n",
      "loss:  0.005193592936239981\n",
      "iteration  20\n",
      "loss:  0.006605389064715354\n",
      "loss:  0.0041573050218406925\n",
      "iteration  21\n",
      "loss:  0.0055761334103505596\n",
      "loss:  0.0031118718989416776\n",
      "iteration  22\n",
      "loss:  0.004522629089122387\n",
      "loss:  0.0025234547460735476\n",
      "iteration  23\n",
      "loss:  0.003667907774029569\n",
      "loss:  0.002046132475240997\n",
      "iteration  24\n",
      "loss:  0.0029744601968077896\n",
      "loss:  0.0016591080738756843\n",
      "iteration  25\n",
      "loss:  0.0024119465953933786\n",
      "loss:  0.0013453227906553293\n",
      "iteration  26\n",
      "loss:  0.001955710206115916\n",
      "loss:  0.0010909334346846796\n",
      "iteration  27\n",
      "loss:  0.0015857195410828432\n",
      "loss:  0.0008847081860701291\n",
      "iteration  28\n",
      "loss:  0.0012857056225651338\n",
      "loss:  0.0007175354674034875\n",
      "iteration  29\n",
      "loss:  0.0010424588257787514\n",
      "loss:  0.0005820247729334972\n",
      "iteration  30\n",
      "loss:  0.0008452561686098363\n",
      "iteration  31\n",
      "loss:  0.0008452561686098363\n",
      "iteration  32\n",
      "loss:  0.0008452561686098363\n",
      "iteration  33\n",
      "loss:  0.0008452561686098363\n",
      "iteration  34\n",
      "loss:  0.0008452561686098363\n",
      "iteration  35\n",
      "loss:  0.0008452561686098363\n",
      "iteration  36\n",
      "loss:  0.0008452561686098363\n",
      "iteration  37\n",
      "loss:  0.0008452561686098363\n",
      "iteration  38\n",
      "loss:  0.0008452561686098363\n",
      "iteration  39\n",
      "loss:  0.0008452561686098363\n",
      "start_i: 130/276\n",
      "start:\n",
      "[ 0.65259933  1.9798238  -0.6514962   1.44679102]\n",
      "goal:\n",
      "[ 0.37708495  2.39737728 -2.1944163   2.74848579]\n",
      "iteration  0\n",
      "loss:  0.2933984774312425\n",
      "loss:  0.29320106850427563\n",
      "iteration  1\n",
      "loss:  0.29320106850427563\n",
      "loss:  0.23880528279467828\n",
      "iteration  2\n",
      "loss:  0.24341783986977378\n",
      "loss:  0.1941587450588538\n",
      "iteration  3\n",
      "loss:  0.19898050696819464\n",
      "loss:  0.15802315467662073\n",
      "iteration  4\n",
      "loss:  0.1620491900324311\n",
      "loss:  0.12859943655419753\n",
      "iteration  5\n",
      "loss:  0.13413983354288442\n",
      "loss:  0.10415982971752927\n",
      "iteration  6\n",
      "loss:  0.1097389300071997\n",
      "loss:  0.08453579940446601\n",
      "iteration  7\n",
      "loss:  0.0892097060543452\n",
      "loss:  0.06869947757873056\n",
      "iteration  8\n",
      "loss:  0.0724964149092019\n",
      "loss:  0.05581284572621938\n",
      "iteration  9\n",
      "loss:  0.05889616161635686\n",
      "loss:  0.045331748870493065\n",
      "iteration  10\n",
      "loss:  0.04853978006689692\n",
      "loss:  0.03660696862193433\n",
      "iteration  11\n",
      "loss:  0.03999210302236174\n",
      "loss:  0.029509131857718522\n",
      "iteration  12\n",
      "loss:  0.03246955411683464\n",
      "loss:  0.023954782582379486\n",
      "iteration  13\n",
      "loss:  0.02635809162887344\n",
      "loss:  0.01944352845888092\n",
      "iteration  14\n",
      "loss:  0.0216815304893148\n",
      "loss:  0.015649540961057785\n",
      "iteration  15\n",
      "loss:  0.017805438670598975\n",
      "loss:  0.012587101897231546\n",
      "iteration  16\n",
      "loss:  0.014838635097536983\n",
      "loss:  0.009938178472162388\n",
      "iteration  17\n",
      "loss:  0.012124147325445252\n",
      "loss:  0.00799582383126153\n",
      "iteration  18\n",
      "loss:  0.009829714269929337\n",
      "loss:  0.0064870701860137185\n",
      "iteration  19\n",
      "loss:  0.007969098353794182\n",
      "loss:  0.005262895314391247\n",
      "iteration  20\n",
      "loss:  0.006460421722025423\n",
      "loss:  0.00426976962504524\n",
      "iteration  21\n",
      "loss:  0.00523724440392544\n",
      "loss:  0.0034641701935921375\n",
      "iteration  22\n",
      "loss:  0.00424563316419495\n",
      "loss:  0.0028107454926651603\n",
      "iteration  23\n",
      "loss:  0.0034418171228226572\n",
      "loss:  0.002280788984615072\n",
      "iteration  24\n",
      "loss:  0.0027902800888408995\n",
      "loss:  0.0018509956753820788\n",
      "iteration  25\n",
      "loss:  0.002262209062370669\n",
      "loss:  0.0015024497469183057\n",
      "iteration  26\n",
      "loss:  0.0018342328178824337\n",
      "loss:  0.0012198008598206479\n",
      "iteration  27\n",
      "loss:  0.0014873968856147894\n",
      "loss:  0.000990594170033519\n",
      "iteration  28\n",
      "loss:  0.0012063308085177016\n",
      "loss:  0.0008047256788776512\n",
      "iteration  29\n",
      "loss:  0.00097857191953783\n",
      "iteration  30\n",
      "loss:  0.00097857191953783\n",
      "iteration  31\n",
      "loss:  0.00097857191953783\n",
      "iteration  32\n",
      "loss:  0.00097857191953783\n",
      "iteration  33\n",
      "loss:  0.00097857191953783\n",
      "iteration  34\n",
      "loss:  0.00097857191953783\n",
      "iteration  35\n",
      "loss:  0.00097857191953783\n",
      "iteration  36\n",
      "loss:  0.00097857191953783\n",
      "iteration  37\n",
      "loss:  0.00097857191953783\n",
      "iteration  38\n",
      "loss:  0.00097857191953783\n",
      "iteration  39\n",
      "loss:  0.00097857191953783\n",
      "start_i: 140/276\n",
      "start:\n",
      "[ 0.37708495  2.39737728 -2.1944163   2.74848579]\n",
      "goal:\n",
      "[-0.15254693  3.00978265 -2.98471039  3.32206157]\n",
      "iteration  0\n",
      "loss:  0.2004887376915922\n",
      "loss:  0.2003750583637432\n",
      "iteration  1\n",
      "loss:  0.2003750583637432\n",
      "loss:  0.16253331654009692\n",
      "iteration  2\n",
      "loss:  0.1670829294988784\n",
      "loss:  0.13125504119486966\n",
      "iteration  3\n",
      "loss:  0.13808518622862145\n",
      "loss:  0.10588395873337578\n",
      "iteration  4\n",
      "loss:  0.11365359404033151\n",
      "loss:  0.0853669982321862\n",
      "iteration  5\n",
      "loss:  0.09346286623962495\n",
      "loss:  0.06873145660308948\n",
      "iteration  6\n",
      "loss:  0.07578190431353757\n",
      "loss:  0.05572045259309049\n",
      "iteration  7\n",
      "loss:  0.06143901621159638\n",
      "loss:  0.04516811747233555\n",
      "iteration  8\n",
      "loss:  0.04980565221911853\n",
      "loss:  0.03661140225597616\n",
      "iteration  9\n",
      "loss:  0.04037155939235648\n",
      "loss:  0.02967382301034405\n",
      "iteration  10\n",
      "loss:  0.03272208687863194\n",
      "loss:  0.024049642181650656\n",
      "iteration  11\n",
      "loss:  0.026520424097318772\n",
      "loss:  0.01949065973335192\n",
      "iteration  12\n",
      "loss:  0.022066594868822894\n",
      "loss:  0.01549283404000851\n",
      "iteration  13\n",
      "loss:  0.01800772981289858\n",
      "loss:  0.01248638140585767\n",
      "iteration  14\n",
      "loss:  0.014595150450388186\n",
      "loss:  0.010119448330222644\n",
      "iteration  15\n",
      "loss:  0.012092416546757528\n",
      "loss:  0.00801330655180105\n",
      "iteration  16\n",
      "loss:  0.009952804785829172\n",
      "loss:  0.006370083979902064\n",
      "iteration  17\n",
      "loss:  0.008065539840737341\n",
      "loss:  0.005162876537178547\n",
      "iteration  18\n",
      "loss:  0.006536247826198674\n",
      "loss:  0.00418466520842029\n",
      "iteration  19\n",
      "loss:  0.005297061397462099\n",
      "loss:  0.0033920440054250583\n",
      "iteration  20\n",
      "loss:  0.004292983587680834\n",
      "loss:  0.002749812338334263\n",
      "iteration  21\n",
      "loss:  0.003479432272493243\n",
      "loss:  0.002229441480775965\n",
      "iteration  22\n",
      "loss:  0.002820271170626697\n",
      "loss:  0.0018078113942329842\n",
      "iteration  23\n",
      "loss:  0.0022862121357015066\n",
      "loss:  0.0014661862971652158\n",
      "iteration  24\n",
      "loss:  0.001853519016275618\n",
      "loss:  0.001189383972596718\n",
      "iteration  25\n",
      "loss:  0.0015029563580778687\n",
      "loss:  0.0009651022635701902\n",
      "iteration  26\n",
      "loss:  0.0012189368401246984\n",
      "loss:  0.0007833730705079111\n",
      "iteration  27\n",
      "loss:  0.0009888299708285015\n",
      "iteration  28\n",
      "loss:  0.0009888299708285015\n",
      "iteration  29\n",
      "loss:  0.0009888299708285015\n",
      "iteration  30\n",
      "loss:  0.0009888299708285015\n",
      "iteration  31\n",
      "loss:  0.0009888299708285015\n",
      "iteration  32\n",
      "loss:  0.0009888299708285015\n",
      "iteration  33\n",
      "loss:  0.0009888299708285015\n",
      "iteration  34\n",
      "loss:  0.0009888299708285015\n",
      "iteration  35\n",
      "loss:  0.0009888299708285015\n",
      "iteration  36\n",
      "loss:  0.0009888299708285015\n",
      "iteration  37\n",
      "loss:  0.0009888299708285015\n",
      "iteration  38\n",
      "loss:  0.0009888299708285015\n",
      "iteration  39\n",
      "loss:  0.0009888299708285015\n",
      "start_i: 150/276\n",
      "start:\n",
      "[-0.15254693  3.00978265 -2.98471039  3.32206157]\n",
      "goal:\n",
      "[-0.70184331 -2.63167733 -2.0821338   2.67811133]\n",
      "iteration  0\n",
      "loss:  0.0060092833984646855\n",
      "loss:  0.006006176363224688\n",
      "iteration  1\n",
      "loss:  0.006006176363224688\n",
      "loss:  0.004967647495976667\n",
      "iteration  2\n",
      "loss:  0.004967647495976667\n",
      "loss:  0.004125830361347282\n",
      "iteration  3\n",
      "loss:  0.004125830361347282\n",
      "loss:  0.0034435083177133813\n",
      "iteration  4\n",
      "loss:  0.0034435083177133813\n",
      "loss:  0.0028902801383337215\n",
      "iteration  5\n",
      "loss:  0.0028902801383337215\n",
      "loss:  0.002441547997043555\n",
      "iteration  6\n",
      "loss:  0.002441547997043555\n",
      "loss:  0.0020774076188138776\n",
      "iteration  7\n",
      "loss:  0.0020774076188138776\n",
      "loss:  0.0017817511409469358\n",
      "iteration  8\n",
      "loss:  0.0017817511409469358\n",
      "loss:  0.001541541836283248\n",
      "iteration  9\n",
      "loss:  0.001541541836283248\n",
      "loss:  0.0013462276708827528\n",
      "iteration  10\n",
      "loss:  0.0013462276708827528\n",
      "loss:  0.0011872670413714563\n",
      "iteration  11\n",
      "loss:  0.0011872670413714563\n",
      "loss:  0.0010577448077095952\n",
      "iteration  12\n",
      "loss:  0.0010577448077095952\n",
      "loss:  0.0009520616563897862\n",
      "iteration  13\n",
      "loss:  0.0009520616563897862\n",
      "iteration  14\n",
      "loss:  0.0009520616563897862\n",
      "iteration  15\n",
      "loss:  0.0009520616563897862\n",
      "iteration  16\n",
      "loss:  0.0009520616563897862\n",
      "iteration  17\n",
      "loss:  0.0009520616563897862\n",
      "iteration  18\n",
      "loss:  0.0009520616563897862\n",
      "iteration  19\n",
      "loss:  0.0009520616563897862\n",
      "iteration  20\n",
      "loss:  0.0009520616563897862\n",
      "iteration  21\n",
      "loss:  0.0009520616563897862\n",
      "iteration  22\n",
      "loss:  0.0009520616563897862\n",
      "iteration  23\n",
      "loss:  0.0009520616563897862\n",
      "iteration  24\n",
      "loss:  0.0009520616563897862\n",
      "iteration  25\n",
      "loss:  0.0009520616563897862\n",
      "iteration  26\n",
      "loss:  0.0009520616563897862\n",
      "iteration  27\n",
      "loss:  0.0009520616563897862\n",
      "iteration  28\n",
      "loss:  0.0009520616563897862\n",
      "iteration  29\n",
      "loss:  0.0009520616563897862\n",
      "iteration  30\n",
      "loss:  0.0009520616563897862\n",
      "iteration  31\n",
      "loss:  0.0009520616563897862\n",
      "iteration  32\n",
      "loss:  0.0009520616563897862\n",
      "iteration  33\n",
      "loss:  0.0009520616563897862\n",
      "iteration  34\n",
      "loss:  0.0009520616563897862\n",
      "iteration  35\n",
      "loss:  0.0009520616563897862\n",
      "iteration  36\n",
      "loss:  0.0009520616563897862\n",
      "iteration  37\n",
      "loss:  0.0009520616563897862\n",
      "iteration  38\n",
      "loss:  0.0009520616563897862\n",
      "iteration  39\n",
      "loss:  0.0009520616563897862\n",
      "start_i: 160/276\n",
      "start:\n",
      "[-0.70184331 -2.63167733 -2.0821338   2.67811133]\n",
      "goal:\n",
      "[-0.95057397 -2.24851539 -0.10975265  0.76233926]\n",
      "iteration  0\n",
      "loss:  0.1821804767377815\n",
      "loss:  0.18205568204012654\n",
      "iteration  1\n",
      "loss:  0.18205568204012654\n",
      "loss:  0.1481641706329485\n",
      "iteration  2\n",
      "loss:  0.14967316445854265\n",
      "loss:  0.12053893030055379\n",
      "iteration  3\n",
      "loss:  0.12335759017689026\n",
      "loss:  0.09791293763272188\n",
      "iteration  4\n",
      "loss:  0.10278950443873681\n",
      "loss:  0.07909572158409801\n",
      "iteration  5\n",
      "loss:  0.08375699523794627\n",
      "loss:  0.06427107106293316\n",
      "iteration  6\n",
      "loss:  0.06811672909540623\n",
      "loss:  0.0522294005111963\n",
      "iteration  7\n",
      "loss:  0.05537524442391485\n",
      "loss:  0.042430102529077335\n",
      "iteration  8\n",
      "loss:  0.04500123817668159\n",
      "loss:  0.03445954358185488\n",
      "iteration  9\n",
      "loss:  0.03655931356234169\n",
      "loss:  0.027979268567284728\n",
      "iteration  10\n",
      "loss:  0.02969290828616935\n",
      "loss:  0.02271268648645097\n",
      "iteration  11\n",
      "loss:  0.024655092405796587\n",
      "loss:  0.018237863586468093\n",
      "iteration  12\n",
      "loss:  0.02013528739367494\n",
      "loss:  0.014752560577170806\n",
      "iteration  13\n",
      "loss:  0.016343788940112406\n",
      "loss:  0.01196928555439356\n",
      "iteration  14\n",
      "loss:  0.013264271781379584\n",
      "loss:  0.00971005708411939\n",
      "iteration  15\n",
      "loss:  0.010763636303925572\n",
      "loss:  0.007876647988298591\n",
      "iteration  16\n",
      "loss:  0.008733575092933852\n",
      "loss:  0.006389092199744695\n",
      "iteration  17\n",
      "loss:  0.007085904000091467\n",
      "loss:  0.005182360640073107\n",
      "iteration  18\n",
      "loss:  0.005809008960597098\n",
      "loss:  0.004176495609743193\n",
      "iteration  19\n",
      "loss:  0.004833510165645165\n",
      "loss:  0.0033143752655284573\n",
      "iteration  20\n",
      "loss:  0.003922426675927537\n",
      "loss:  0.0026887815199625603\n",
      "iteration  21\n",
      "loss:  0.0031834214596574545\n",
      "loss:  0.002181600015138269\n",
      "iteration  22\n",
      "loss:  0.002584020101348765\n",
      "loss:  0.001770493020441326\n",
      "iteration  23\n",
      "loss:  0.002097906604287129\n",
      "loss:  0.0014372923826417327\n",
      "iteration  24\n",
      "loss:  0.001703708560094643\n",
      "loss:  0.0011672568761020602\n",
      "iteration  25\n",
      "loss:  0.001384073178888441\n",
      "loss:  0.0009484280571063859\n",
      "iteration  26\n",
      "loss:  0.0011249147038118385\n",
      "loss:  0.0007711066039444405\n",
      "iteration  27\n",
      "loss:  0.0009148017978969115\n",
      "iteration  28\n",
      "loss:  0.0009148017978969115\n",
      "iteration  29\n",
      "loss:  0.0009148017978969115\n",
      "iteration  30\n",
      "loss:  0.0009148017978969115\n",
      "iteration  31\n",
      "loss:  0.0009148017978969115\n",
      "iteration  32\n",
      "loss:  0.0009148017978969115\n",
      "iteration  33\n",
      "loss:  0.0009148017978969115\n",
      "iteration  34\n",
      "loss:  0.0009148017978969115\n",
      "iteration  35\n",
      "loss:  0.0009148017978969115\n",
      "iteration  36\n",
      "loss:  0.0009148017978969115\n",
      "iteration  37\n",
      "loss:  0.0009148017978969115\n",
      "iteration  38\n",
      "loss:  0.0009148017978969115\n",
      "iteration  39\n",
      "loss:  0.0009148017978969115\n",
      "start_i: 170/276\n",
      "start:\n",
      "[-0.95057397 -2.24851539 -0.10975265  0.76233926]\n",
      "goal:\n",
      "[-0.78092116 -2.29690875  1.98748006 -1.37821222]\n",
      "iteration  0\n",
      "loss:  0.30765865941524734\n",
      "loss:  0.3074415456377782\n",
      "iteration  1\n",
      "loss:  0.3074415456377782\n",
      "loss:  0.2505327230643213\n",
      "iteration  2\n",
      "loss:  0.2505327230643213\n",
      "loss:  0.2042007602666187\n",
      "iteration  3\n",
      "loss:  0.20945816666367542\n",
      "loss:  0.1656885007867075\n",
      "iteration  4\n",
      "loss:  0.1741556771263838\n",
      "loss:  0.1340914060331813\n",
      "iteration  5\n",
      "loss:  0.1418172922972468\n",
      "loss:  0.1091066570369646\n",
      "iteration  6\n",
      "loss:  0.11542814198610019\n",
      "loss:  0.08873412865578788\n",
      "iteration  7\n",
      "loss:  0.09390270158587662\n",
      "loss:  0.07213572657209341\n",
      "iteration  8\n",
      "loss:  0.07635743364909525\n",
      "loss:  0.058620510501074946\n",
      "iteration  9\n",
      "loss:  0.062065730279796484\n",
      "loss:  0.04762175859884971\n",
      "iteration  10\n",
      "loss:  0.050431051898789145\n",
      "loss:  0.038675264765999834\n",
      "iteration  11\n",
      "loss:  0.04096436079132071\n",
      "loss:  0.03140125428339088\n",
      "iteration  12\n",
      "loss:  0.03358342732279152\n",
      "loss:  0.025401486153152134\n",
      "iteration  13\n",
      "loss:  0.02783143426580911\n",
      "loss:  0.020393445459487347\n",
      "iteration  14\n",
      "loss:  0.022592306485602356\n",
      "loss:  0.016547095192482073\n",
      "iteration  15\n",
      "loss:  0.018335923216619046\n",
      "loss:  0.01342372214987568\n",
      "iteration  16\n",
      "loss:  0.014878492806953263\n",
      "loss:  0.010888277478542568\n",
      "iteration  17\n",
      "loss:  0.012070857257273939\n",
      "loss:  0.008830546366726168\n",
      "iteration  18\n",
      "loss:  0.009791482767133158\n",
      "loss:  0.007160850701186227\n",
      "iteration  19\n",
      "loss:  0.007941405267182877\n",
      "loss:  0.005806256493606327\n",
      "iteration  20\n",
      "loss:  0.006440083305019515\n",
      "loss:  0.004707472434742862\n",
      "iteration  21\n",
      "loss:  0.005381967454480221\n",
      "loss:  0.003723195425108846\n",
      "iteration  22\n",
      "loss:  0.004380247169525693\n",
      "loss:  0.0030081817389102854\n",
      "iteration  23\n",
      "loss:  0.003551244171283506\n",
      "loss:  0.0024382803386560035\n",
      "iteration  24\n",
      "loss:  0.0029763856727170316\n",
      "loss:  0.0018947459717890587\n",
      "iteration  25\n",
      "loss:  0.0026411275001404\n",
      "loss:  0.0011249265633589663\n",
      "iteration  26\n",
      "loss:  0.0021460035978448267\n",
      "loss:  0.0009031166842558383\n",
      "iteration  27\n",
      "loss:  0.0017394767309299192\n",
      "loss:  0.0007317058308191273\n",
      "iteration  28\n",
      "loss:  0.001409873051844264\n",
      "loss:  0.0005928439378162913\n",
      "iteration  29\n",
      "loss:  0.0011426617130202092\n",
      "loss:  0.0004803520151208544\n",
      "iteration  30\n",
      "loss:  0.0009713315006785828\n",
      "iteration  31\n",
      "loss:  0.0009713315006785828\n",
      "iteration  32\n",
      "loss:  0.0009713315006785828\n",
      "iteration  33\n",
      "loss:  0.0009713315006785828\n",
      "iteration  34\n",
      "loss:  0.0009713315006785828\n",
      "iteration  35\n",
      "loss:  0.0009713315006785828\n",
      "iteration  36\n",
      "loss:  0.0009713315006785828\n",
      "iteration  37\n",
      "loss:  0.0009713315006785828\n",
      "iteration  38\n",
      "loss:  0.0009713315006785828\n",
      "iteration  39\n",
      "loss:  0.0009713315006785828\n",
      "start_i: 180/276\n",
      "start:\n",
      "[-0.78092116 -2.29690875  1.98748006 -1.37821222]\n",
      "goal:\n",
      "[-0.22358253 -2.70337327  3.60139974 -2.61910037]\n",
      "iteration  0\n",
      "loss:  0.24135827192358267\n",
      "loss:  0.24120813993519474\n",
      "iteration  1\n",
      "loss:  0.24120813993519474\n",
      "loss:  0.19603852086020407\n",
      "iteration  2\n",
      "loss:  0.19603852086020407\n",
      "loss:  0.15934529805826197\n",
      "iteration  3\n",
      "loss:  0.15934529805826197\n",
      "loss:  0.12947027664753652\n",
      "iteration  4\n",
      "loss:  0.12947027664753652\n",
      "loss:  0.10516081731120396\n",
      "iteration  5\n",
      "loss:  0.10552871096444577\n",
      "loss:  0.08538311205437524\n",
      "iteration  6\n",
      "loss:  0.08726043896262267\n",
      "loss:  0.06915180342859677\n",
      "iteration  7\n",
      "loss:  0.07182885960895886\n",
      "loss:  0.05595663213949898\n",
      "iteration  8\n",
      "loss:  0.058958263502302544\n",
      "loss:  0.04525753347144258\n",
      "iteration  9\n",
      "loss:  0.04783964051662676\n",
      "loss:  0.03671564843545141\n",
      "iteration  10\n",
      "loss:  0.03881159837001413\n",
      "loss:  0.029781316993413645\n",
      "iteration  11\n",
      "loss:  0.03148250070054391\n",
      "loss:  0.024153455050487344\n",
      "iteration  12\n",
      "loss:  0.026222497485968808\n",
      "loss:  0.019335268713780013\n",
      "iteration  13\n",
      "loss:  0.021286760267228307\n",
      "loss:  0.015670569490537196\n",
      "iteration  14\n",
      "loss:  0.017262034963360748\n",
      "loss:  0.012705680659116595\n",
      "iteration  15\n",
      "loss:  0.014430079853275563\n",
      "loss:  0.010060351052820883\n",
      "iteration  16\n",
      "loss:  0.012143444058937374\n",
      "loss:  0.007794504187310943\n",
      "iteration  17\n",
      "loss:  0.010140715563225922\n",
      "loss:  0.006015989972755277\n",
      "iteration  18\n",
      "loss:  0.008367631503561603\n",
      "loss:  0.004709951969407789\n",
      "iteration  19\n",
      "loss:  0.006784330246288028\n",
      "loss:  0.003818272414838353\n",
      "iteration  20\n",
      "loss:  0.005500698605446691\n",
      "loss:  0.003095564517026224\n",
      "iteration  21\n",
      "loss:  0.004460071634910921\n",
      "loss:  0.0025098961958812003\n",
      "iteration  22\n",
      "loss:  0.0036165152288203757\n",
      "loss:  0.0020353102241014893\n",
      "iteration  23\n",
      "loss:  0.002998346132486334\n",
      "loss:  0.0015638504718477398\n",
      "iteration  24\n",
      "loss:  0.002522503491802005\n",
      "loss:  0.0011191029377764135\n",
      "iteration  25\n",
      "loss:  0.0020497633752448855\n",
      "loss:  0.000900971770625111\n",
      "iteration  26\n",
      "loss:  0.0017826733766282267\n",
      "loss:  0.0004492255655009772\n",
      "iteration  27\n",
      "loss:  0.0014446056136497702\n",
      "loss:  0.0003645514546917574\n",
      "iteration  28\n",
      "loss:  0.001170641164723331\n",
      "loss:  0.0002958742030645828\n",
      "iteration  29\n",
      "loss:  0.0009486155503853792\n",
      "iteration  30\n",
      "loss:  0.0009486155503853792\n",
      "iteration  31\n",
      "loss:  0.0009486155503853792\n",
      "iteration  32\n",
      "loss:  0.0009486155503853792\n",
      "iteration  33\n",
      "loss:  0.0009486155503853792\n",
      "iteration  34\n",
      "loss:  0.0009486155503853792\n",
      "iteration  35\n",
      "loss:  0.0009486155503853792\n",
      "iteration  36\n",
      "loss:  0.0009486155503853792\n",
      "iteration  37\n",
      "loss:  0.0009486155503853792\n",
      "iteration  38\n",
      "loss:  0.0009486155503853792\n",
      "iteration  39\n",
      "loss:  0.0009486155503853792\n",
      "start_i: 190/276\n",
      "start:\n",
      "[-0.22358253 -2.70337327  3.60139974 -2.61910037]\n",
      "goal:\n",
      "[ 0.54388345  3.02133017  3.78094082 -2.86894557]\n",
      "iteration  0\n",
      "loss:  0.40502010619150186\n",
      "loss:  0.4047912484395499\n",
      "iteration  1\n",
      "loss:  0.4047912484395499\n",
      "loss:  0.32813774082902925\n",
      "iteration  2\n",
      "loss:  0.32813774082902925\n",
      "loss:  0.26599373449614616\n",
      "iteration  3\n",
      "loss:  0.26599373449614616\n",
      "loss:  0.2155832140677766\n",
      "iteration  4\n",
      "loss:  0.2155832140677766\n",
      "loss:  0.17470349746535851\n",
      "iteration  5\n",
      "loss:  0.17470349746535851\n",
      "loss:  0.14156091092506334\n",
      "iteration  6\n",
      "loss:  0.14156091092506334\n",
      "loss:  0.1146964334791368\n",
      "iteration  7\n",
      "loss:  0.1146964334791368\n",
      "loss:  0.09292428101491453\n",
      "iteration  8\n",
      "loss:  0.09292428101491453\n",
      "loss:  0.07528140332963887\n",
      "iteration  9\n",
      "loss:  0.07528140332963887\n",
      "loss:  0.06098608150018823\n",
      "iteration  10\n",
      "loss:  0.06098608150018823\n",
      "loss:  0.04940406834206351\n",
      "iteration  11\n",
      "loss:  0.05037037336966608\n",
      "loss:  0.039936974029591954\n",
      "iteration  12\n",
      "loss:  0.04140932148719087\n",
      "loss:  0.03225656011026598\n",
      "iteration  13\n",
      "loss:  0.034307790090659915\n",
      "loss:  0.02592186628696591\n",
      "iteration  14\n",
      "loss:  0.028867905272749725\n",
      "loss:  0.020459213024838403\n",
      "iteration  15\n",
      "loss:  0.024188300741219252\n",
      "loss:  0.01599180820756633\n",
      "iteration  16\n",
      "loss:  0.019865152758221347\n",
      "loss:  0.012733668469023098\n",
      "iteration  17\n",
      "loss:  0.01609074183019495\n",
      "loss:  0.010313188898267274\n",
      "iteration  18\n",
      "loss:  0.013033466887370598\n",
      "loss:  0.008352748530605258\n",
      "iteration  19\n",
      "loss:  0.010557108579127797\n",
      "loss:  0.006764930697906812\n",
      "iteration  20\n",
      "loss:  0.008551307319508534\n",
      "loss:  0.005478924347311298\n",
      "iteration  21\n",
      "loss:  0.006926659394944319\n",
      "loss:  0.004437371947628127\n",
      "iteration  22\n",
      "loss:  0.0056107416446639835\n",
      "loss:  0.003593814371835564\n",
      "iteration  23\n",
      "loss:  0.004544888741918115\n",
      "loss:  0.0029106207111305317\n",
      "iteration  24\n",
      "loss:  0.0038310639939933937\n",
      "loss:  0.0021898031793484447\n",
      "iteration  25\n",
      "loss:  0.003118419457600599\n",
      "loss:  0.0017564247258153064\n",
      "iteration  26\n",
      "loss:  0.0025258348188575966\n",
      "loss:  0.0014226961689868694\n",
      "iteration  27\n",
      "loss:  0.0020458623498639315\n",
      "loss:  0.0011523779699832018\n",
      "iteration  28\n",
      "loss:  0.0016571008501524813\n",
      "loss:  0.0009334215747904204\n",
      "iteration  29\n",
      "loss:  0.001342216263748945\n",
      "loss:  0.0007560679763121191\n",
      "iteration  30\n",
      "loss:  0.0010871689373996682\n",
      "loss:  0.0006124124183346406\n",
      "iteration  31\n",
      "loss:  0.000880587481093373\n",
      "iteration  32\n",
      "loss:  0.000880587481093373\n",
      "iteration  33\n",
      "loss:  0.000880587481093373\n",
      "iteration  34\n",
      "loss:  0.000880587481093373\n",
      "iteration  35\n",
      "loss:  0.000880587481093373\n",
      "iteration  36\n",
      "loss:  0.000880587481093373\n",
      "iteration  37\n",
      "loss:  0.000880587481093373\n",
      "iteration  38\n",
      "loss:  0.000880587481093373\n",
      "iteration  39\n",
      "loss:  0.000880587481093373\n",
      "start_i: 200/276\n",
      "start:\n",
      "[ 0.54388345  3.02133017  3.78094082 -2.86894557]\n",
      "goal:\n",
      "[ 1.19335262  2.47075519  2.42764767 -2.50752834]\n",
      "iteration  0\n",
      "loss:  0.30339126102462954\n",
      "loss:  0.3032138072654943\n",
      "iteration  1\n",
      "loss:  0.3032138072654943\n",
      "loss:  0.24466362558444965\n",
      "iteration  2\n",
      "loss:  0.24466362558444965\n",
      "loss:  0.1973775583257037\n",
      "iteration  3\n",
      "loss:  0.1973775583257037\n",
      "loss:  0.1592855335413344\n",
      "iteration  4\n",
      "loss:  0.1592855335413344\n",
      "loss:  0.12858688158582993\n",
      "iteration  5\n",
      "loss:  0.12858688158582993\n",
      "loss:  0.10383638903112119\n",
      "iteration  6\n",
      "loss:  0.10415144317334578\n",
      "loss:  0.0838703373022524\n",
      "iteration  7\n",
      "loss:  0.08578646583233765\n",
      "loss:  0.06760301294044987\n",
      "iteration  8\n",
      "loss:  0.0710942777688597\n",
      "loss:  0.05424025663528061\n",
      "iteration  9\n",
      "loss:  0.059063726013364425\n",
      "loss:  0.04324915788496074\n",
      "iteration  10\n",
      "loss:  0.04821429007634722\n",
      "loss:  0.034770805521166424\n",
      "iteration  11\n",
      "loss:  0.039909569969164393\n",
      "loss:  0.027599288826840835\n",
      "iteration  12\n",
      "loss:  0.033035591644650565\n",
      "loss:  0.021782492774409128\n",
      "iteration  13\n",
      "loss:  0.02713252380025265\n",
      "loss:  0.017287843970042745\n",
      "iteration  14\n",
      "loss:  0.02194891954078824\n",
      "loss:  0.013987133011596709\n",
      "iteration  15\n",
      "loss:  0.017757727301574183\n",
      "loss:  0.011318295953184447\n",
      "iteration  16\n",
      "loss:  0.014868824096616226\n",
      "loss:  0.008623889779598009\n",
      "iteration  17\n",
      "loss:  0.01217910948696497\n",
      "loss:  0.006812178059249329\n",
      "iteration  18\n",
      "loss:  0.009857351161011902\n",
      "loss:  0.005512881049667354\n",
      "iteration  19\n",
      "loss:  0.007978770490622024\n",
      "loss:  0.004461854924000216\n",
      "iteration  20\n",
      "loss:  0.006458660549085361\n",
      "loss:  0.0036114964845656624\n",
      "iteration  21\n",
      "loss:  0.0052284971510537\n",
      "loss:  0.002923416278132234\n",
      "iteration  22\n",
      "loss:  0.004232887166655296\n",
      "loss:  0.0023665897251068987\n",
      "iteration  23\n",
      "loss:  0.003427043723327057\n",
      "loss:  0.0019159386976514816\n",
      "iteration  24\n",
      "loss:  0.0027747491474127205\n",
      "loss:  0.0015511876969654553\n",
      "iteration  25\n",
      "loss:  0.002246710776252113\n",
      "loss:  0.001255941156071688\n",
      "iteration  26\n",
      "loss:  0.0018192334652490526\n",
      "loss:  0.0010169388047076868\n",
      "iteration  27\n",
      "loss:  0.0015301015723502224\n",
      "loss:  0.0007395953307045013\n",
      "iteration  28\n",
      "loss:  0.0012638361042195856\n",
      "loss:  0.000559558323264512\n",
      "iteration  29\n",
      "loss:  0.0010234636561833362\n",
      "loss:  0.00045328953203330245\n",
      "iteration  30\n",
      "loss:  0.0008288433558794312\n",
      "iteration  31\n",
      "loss:  0.0008288433558794312\n",
      "iteration  32\n",
      "loss:  0.0008288433558794312\n",
      "iteration  33\n",
      "loss:  0.0008288433558794312\n",
      "iteration  34\n",
      "loss:  0.0008288433558794312\n",
      "iteration  35\n",
      "loss:  0.0008288433558794312\n",
      "iteration  36\n",
      "loss:  0.0008288433558794312\n",
      "iteration  37\n",
      "loss:  0.0008288433558794312\n",
      "iteration  38\n",
      "loss:  0.0008288433558794312\n",
      "iteration  39\n",
      "loss:  0.0008288433558794312\n",
      "start_i: 210/276\n",
      "start:\n",
      "[ 1.19335262  2.47075519  2.42764767 -2.50752834]\n",
      "goal:\n",
      "[ 1.52800844  2.03908498  0.77437127 -1.67356301]\n",
      "iteration  0\n",
      "loss:  0.12742929988890325\n",
      "loss:  0.12733705334945472\n",
      "iteration  1\n",
      "loss:  0.12733705334945472\n",
      "loss:  0.1026904280063774\n",
      "iteration  2\n",
      "loss:  0.10590282625049194\n",
      "loss:  0.08235281884756282\n",
      "iteration  3\n",
      "loss:  0.08678669439722837\n",
      "loss:  0.06614602559377321\n",
      "iteration  4\n",
      "loss:  0.07065548173809913\n",
      "loss:  0.05322617418291159\n",
      "iteration  5\n",
      "loss:  0.058172764443897525\n",
      "loss:  0.042562764829364344\n",
      "iteration  6\n",
      "loss:  0.04697820439632351\n",
      "loss:  0.0343785029225898\n",
      "iteration  7\n",
      "loss:  0.037948479646260164\n",
      "loss:  0.02777752748196056\n",
      "iteration  8\n",
      "loss:  0.03066339909552414\n",
      "loss:  0.022450736229714614\n",
      "iteration  9\n",
      "loss:  0.0258190452014265\n",
      "loss:  0.017498510550000776\n",
      "iteration  10\n",
      "loss:  0.021461580974674724\n",
      "loss:  0.013671930025516737\n",
      "iteration  11\n",
      "loss:  0.017356359441809584\n",
      "loss:  0.01105548131264162\n",
      "iteration  12\n",
      "loss:  0.014038336912271368\n",
      "loss:  0.008942063841709987\n",
      "iteration  13\n",
      "loss:  0.01158393434189453\n",
      "loss:  0.007017874283588016\n",
      "iteration  14\n",
      "loss:  0.009640582873968224\n",
      "loss:  0.005365194583106938\n",
      "iteration  15\n",
      "loss:  0.008229337159903627\n",
      "loss:  0.0036354872753806464\n",
      "iteration  16\n",
      "loss:  0.006659056820447353\n",
      "loss:  0.002943440211671969\n",
      "iteration  17\n",
      "loss:  0.005388919828738847\n",
      "loss:  0.002384013030913809\n",
      "iteration  18\n",
      "loss:  0.004361613525931045\n",
      "loss:  0.0019313571092875494\n",
      "iteration  19\n",
      "loss:  0.0035305914819099204\n",
      "loss:  0.001565026304920678\n",
      "iteration  20\n",
      "loss:  0.002858261633486726\n",
      "loss:  0.0012685058427464929\n",
      "iteration  21\n",
      "loss:  0.002314255139588246\n",
      "loss:  0.0010284521359696644\n",
      "iteration  22\n",
      "loss:  0.0018740321130599285\n",
      "loss:  0.0008340808644761411\n",
      "iteration  23\n",
      "loss:  0.0015177581732575011\n",
      "loss:  0.0006766739286009861\n",
      "iteration  24\n",
      "loss:  0.001229398766999994\n",
      "loss:  0.0005491819691136695\n",
      "iteration  25\n",
      "loss:  0.0009959887231530211\n",
      "iteration  26\n",
      "loss:  0.0009959887231530211\n",
      "iteration  27\n",
      "loss:  0.0009959887231530211\n",
      "iteration  28\n",
      "loss:  0.0009959887231530211\n",
      "iteration  29\n",
      "loss:  0.0009959887231530211\n",
      "iteration  30\n",
      "loss:  0.0009959887231530211\n",
      "iteration  31\n",
      "loss:  0.0009959887231530211\n",
      "iteration  32\n",
      "loss:  0.0009959887231530211\n",
      "iteration  33\n",
      "loss:  0.0009959887231530211\n",
      "iteration  34\n",
      "loss:  0.0009959887231530211\n",
      "iteration  35\n",
      "loss:  0.0009959887231530211\n",
      "iteration  36\n",
      "loss:  0.0009959887231530211\n",
      "iteration  37\n",
      "loss:  0.0009959887231530211\n",
      "iteration  38\n",
      "loss:  0.0009959887231530211\n",
      "iteration  39\n",
      "loss:  0.0009959887231530211\n",
      "start_i: 220/276\n",
      "start:\n",
      "[ 1.52800844  2.03908498  0.77437127 -1.67356301]\n",
      "goal:\n",
      "[ 1.46289971  1.96815658 -1.66990088  1.22434561]\n",
      "iteration  0\n",
      "loss:  0.15026020079677968\n",
      "loss:  0.15013611626615483\n",
      "iteration  1\n",
      "loss:  0.15013611626615483\n",
      "loss:  0.12225604555022279\n",
      "iteration  2\n",
      "loss:  0.12394869874074617\n",
      "loss:  0.09947008832964485\n",
      "iteration  3\n",
      "loss:  0.10639751545146575\n",
      "loss:  0.07891812152608849\n",
      "iteration  4\n",
      "loss:  0.0865524265813603\n",
      "loss:  0.0642006932362712\n",
      "iteration  5\n",
      "loss:  0.07038876559331478\n",
      "loss:  0.05220282679173098\n",
      "iteration  6\n",
      "loss:  0.05731909167565555\n",
      "loss:  0.0423950081883724\n",
      "iteration  7\n",
      "loss:  0.04782531261742093\n",
      "loss:  0.03377563394604721\n",
      "iteration  8\n",
      "loss:  0.03882878944403733\n",
      "loss:  0.027435526243121726\n",
      "iteration  9\n",
      "loss:  0.03151838939300551\n",
      "loss:  0.022278706718662184\n",
      "iteration  10\n",
      "loss:  0.025578805161350207\n",
      "loss:  0.018086725737647868\n",
      "iteration  11\n",
      "loss:  0.02138925711324377\n",
      "loss:  0.014233470111732885\n",
      "iteration  12\n",
      "loss:  0.017530036688234107\n",
      "loss:  0.011420523762081435\n",
      "iteration  13\n",
      "loss:  0.014223551970742057\n",
      "loss:  0.009268448623972678\n",
      "iteration  14\n",
      "loss:  0.011539204669117786\n",
      "loss:  0.007520963023908041\n",
      "iteration  15\n",
      "loss:  0.009360323533929122\n",
      "loss:  0.006102417635615565\n",
      "iteration  16\n",
      "loss:  0.00759210378092538\n",
      "loss:  0.004951122578961015\n",
      "iteration  17\n",
      "loss:  0.006157419785290468\n",
      "loss:  0.0040168932098673515\n",
      "iteration  18\n",
      "loss:  0.004993556286157066\n",
      "loss:  0.003258921941968139\n",
      "iteration  19\n",
      "loss:  0.004049536161946241\n",
      "loss:  0.002644039032183197\n",
      "iteration  20\n",
      "loss:  0.0032839382705461308\n",
      "loss:  0.0021452923614477606\n",
      "iteration  21\n",
      "loss:  0.0026940573797556745\n",
      "loss:  0.001714811832349938\n",
      "iteration  22\n",
      "loss:  0.0022455384160002885\n",
      "loss:  0.0013277532050906067\n",
      "iteration  23\n",
      "loss:  0.0018589869148988292\n",
      "loss:  0.0010322196903409014\n",
      "iteration  24\n",
      "loss:  0.0015687838439894977\n",
      "loss:  0.0007445350326328487\n",
      "iteration  25\n",
      "loss:  0.0012720665850568962\n",
      "loss:  0.0006050384668910803\n",
      "iteration  26\n",
      "loss:  0.0010315945379271608\n",
      "loss:  0.0004918556840291522\n",
      "iteration  27\n",
      "loss:  0.0008367058613656843\n",
      "iteration  28\n",
      "loss:  0.0008367058613656843\n",
      "iteration  29\n",
      "loss:  0.0008367058613656843\n",
      "iteration  30\n",
      "loss:  0.0008367058613656843\n",
      "iteration  31\n",
      "loss:  0.0008367058613656843\n",
      "iteration  32\n",
      "loss:  0.0008367058613656843\n",
      "iteration  33\n",
      "loss:  0.0008367058613656843\n",
      "iteration  34\n",
      "loss:  0.0008367058613656843\n",
      "iteration  35\n",
      "loss:  0.0008367058613656843\n",
      "iteration  36\n",
      "loss:  0.0008367058613656843\n",
      "iteration  37\n",
      "loss:  0.0008367058613656843\n",
      "iteration  38\n",
      "loss:  0.0008367058613656843\n",
      "iteration  39\n",
      "loss:  0.0008367058613656843\n",
      "start_i: 230/276\n",
      "start:\n",
      "[ 1.46289971  1.96815658 -1.66990088  1.22434561]\n",
      "goal:\n",
      "[ 0.91070545  2.42443555 -4.06275063  3.33502458]\n",
      "iteration  0\n",
      "loss:  0.20794382274851736\n",
      "loss:  0.20779829721598522\n",
      "iteration  1\n",
      "loss:  0.20779829721598522\n",
      "loss:  0.16904066894532335\n",
      "iteration  2\n",
      "loss:  0.16904066894532335\n",
      "loss:  0.13753335615292883\n",
      "iteration  3\n",
      "loss:  0.1380064580934205\n",
      "loss:  0.11184050794776358\n",
      "iteration  4\n",
      "loss:  0.1153028918613283\n",
      "loss:  0.09046927777699329\n",
      "iteration  5\n",
      "loss:  0.0963628035053153\n",
      "loss:  0.0727432549748366\n",
      "iteration  6\n",
      "loss:  0.0813737796530522\n",
      "loss:  0.057460752945336355\n",
      "iteration  7\n",
      "loss:  0.06651881384242891\n",
      "loss:  0.046426047705944655\n",
      "iteration  8\n",
      "loss:  0.054020459122328474\n",
      "loss:  0.037688681190766335\n",
      "iteration  9\n",
      "loss:  0.04385926404456944\n",
      "loss:  0.030588805783040594\n",
      "iteration  10\n",
      "loss:  0.035600943291269206\n",
      "loss:  0.024821673478018436\n",
      "iteration  11\n",
      "loss:  0.028891504571658518\n",
      "loss:  0.0201384594516338\n",
      "iteration  12\n",
      "loss:  0.023442156111597383\n",
      "loss:  0.016336421557218664\n",
      "iteration  13\n",
      "loss:  0.019017480007951823\n",
      "loss:  0.01325046905770776\n",
      "iteration  14\n",
      "loss:  0.015425698936617535\n",
      "loss:  0.010746245370953792\n",
      "iteration  15\n",
      "loss:  0.012535324471157488\n",
      "loss:  0.008700497773083661\n",
      "iteration  16\n",
      "loss:  0.010530828511495456\n",
      "loss:  0.006755766180829586\n",
      "iteration  17\n",
      "loss:  0.008538742625840827\n",
      "loss:  0.005477497532385064\n",
      "iteration  18\n",
      "loss:  0.006923014759525125\n",
      "loss:  0.004440707632089702\n",
      "iteration  19\n",
      "loss:  0.005612566198771585\n",
      "loss:  0.0036000046314686686\n",
      "iteration  20\n",
      "loss:  0.004549857242514954\n",
      "loss:  0.0029183683815610043\n",
      "iteration  21\n",
      "loss:  0.0038158103756796968\n",
      "loss:  0.0022289409077444256\n",
      "iteration  22\n",
      "loss:  0.0031274582674577327\n",
      "loss:  0.0017672811322525833\n",
      "iteration  23\n",
      "loss:  0.0025345844484529743\n",
      "loss:  0.0014324079104162377\n",
      "iteration  24\n",
      "loss:  0.00205402013597244\n",
      "loss:  0.0011609586004075517\n",
      "iteration  25\n",
      "loss:  0.001664510679040263\n",
      "loss:  0.0009409417880375493\n",
      "iteration  26\n",
      "loss:  0.0013488240181473066\n",
      "loss:  0.0007626208534837091\n",
      "iteration  27\n",
      "loss:  0.0010929836207997693\n",
      "loss:  0.0006180998268138889\n",
      "iteration  28\n",
      "loss:  0.0008856549760064155\n",
      "iteration  29\n",
      "loss:  0.0008856549760064155\n",
      "iteration  30\n",
      "loss:  0.0008856549760064155\n",
      "iteration  31\n",
      "loss:  0.0008856549760064155\n",
      "iteration  32\n",
      "loss:  0.0008856549760064155\n",
      "iteration  33\n",
      "loss:  0.0008856549760064155\n",
      "iteration  34\n",
      "loss:  0.0008856549760064155\n",
      "iteration  35\n",
      "loss:  0.0008856549760064155\n",
      "iteration  36\n",
      "loss:  0.0008856549760064155\n",
      "iteration  37\n",
      "loss:  0.0008856549760064155\n",
      "iteration  38\n",
      "loss:  0.0008856549760064155\n",
      "iteration  39\n",
      "loss:  0.0008856549760064155\n",
      "start_i: 240/276\n",
      "start:\n",
      "[ 0.91070545  2.42443555 -4.06275063  3.33502458]\n",
      "goal:\n",
      "[-0.06655723 -3.09644626 -5.51542294  4.24830916]\n",
      "iteration  0\n",
      "loss:  0.1360897528467308\n",
      "loss:  0.13600934852873284\n",
      "iteration  1\n",
      "loss:  0.13600934852873284\n",
      "loss:  0.11024209003702105\n",
      "iteration  2\n",
      "loss:  0.11024209003702105\n",
      "loss:  0.08935821444895732\n",
      "iteration  3\n",
      "loss:  0.08935821444895732\n",
      "loss:  0.07242471622336441\n",
      "iteration  4\n",
      "loss:  0.07404277101058256\n",
      "loss:  0.05853576559728687\n",
      "iteration  5\n",
      "loss:  0.06309678464521538\n",
      "loss:  0.046248173520474604\n",
      "iteration  6\n",
      "loss:  0.052664838716735905\n",
      "loss:  0.03661949554269824\n",
      "iteration  7\n",
      "loss:  0.04268011954648627\n",
      "loss:  0.029675735016809073\n",
      "iteration  8\n",
      "loss:  0.03458798644016476\n",
      "loss:  0.02404861108329595\n",
      "iteration  9\n",
      "loss:  0.02802994998846374\n",
      "loss:  0.0194889059285405\n",
      "iteration  10\n",
      "loss:  0.0229672950652198\n",
      "loss:  0.01563715492237208\n",
      "iteration  11\n",
      "loss:  0.01912231362063436\n",
      "loss:  0.012260779720851019\n",
      "iteration  12\n",
      "loss:  0.015497728640927709\n",
      "loss:  0.009938036484194088\n",
      "iteration  13\n",
      "loss:  0.012561002672818873\n",
      "loss:  0.008056147524559914\n",
      "iteration  14\n",
      "loss:  0.010206909734132815\n",
      "loss:  0.006510788740922398\n",
      "iteration  15\n",
      "loss:  0.008609104571731042\n",
      "loss:  0.004896506044835934\n",
      "iteration  16\n",
      "loss:  0.006978046481722412\n",
      "loss:  0.0039716096176933184\n",
      "iteration  17\n",
      "loss:  0.005656678903866282\n",
      "loss:  0.0032221381498897935\n",
      "iteration  18\n",
      "loss:  0.0045861958890482495\n",
      "loss:  0.0026148296328078135\n",
      "iteration  19\n",
      "loss:  0.0037189664127966785\n",
      "loss:  0.0021227102935877963\n",
      "iteration  20\n",
      "loss:  0.003016398247955052\n",
      "loss:  0.0017239227156250327\n",
      "iteration  21\n",
      "loss:  0.00244722401811505\n",
      "loss:  0.0014007559929936938\n",
      "iteration  22\n",
      "loss:  0.001986111830504994\n",
      "loss:  0.0011388595360903794\n",
      "iteration  23\n",
      "loss:  0.0016125391590580873\n",
      "loss:  0.0009266058059952315\n",
      "iteration  24\n",
      "loss:  0.0013098801851107794\n",
      "loss:  0.0007545738110986135\n",
      "iteration  25\n",
      "loss:  0.0010646661998529694\n",
      "loss:  0.0006151305066175891\n",
      "iteration  26\n",
      "loss:  0.0008659862964467204\n",
      "iteration  27\n",
      "loss:  0.0008659862964467204\n",
      "iteration  28\n",
      "loss:  0.0008659862964467204\n",
      "iteration  29\n",
      "loss:  0.0008659862964467204\n",
      "iteration  30\n",
      "loss:  0.0008659862964467204\n",
      "iteration  31\n",
      "loss:  0.0008659862964467204\n",
      "iteration  32\n",
      "loss:  0.0008659862964467204\n",
      "iteration  33\n",
      "loss:  0.0008659862964467204\n",
      "iteration  34\n",
      "loss:  0.0008659862964467204\n",
      "iteration  35\n",
      "loss:  0.0008659862964467204\n",
      "iteration  36\n",
      "loss:  0.0008659862964467204\n",
      "iteration  37\n",
      "loss:  0.0008659862964467204\n",
      "iteration  38\n",
      "loss:  0.0008659862964467204\n",
      "iteration  39\n",
      "loss:  0.0008659862964467204\n",
      "start_i: 250/276\n",
      "start:\n",
      "[-0.06655723 -3.09644626 -5.51542294  4.24830916]\n",
      "goal:\n",
      "[-1.10505773 -2.19351531 -4.41083422  4.77577156]\n",
      "iteration  0\n",
      "loss:  0.2603240341871137\n",
      "loss:  0.2601570570438408\n",
      "iteration  1\n",
      "loss:  0.2601570570438408\n",
      "loss:  0.20964482508880306\n",
      "iteration  2\n",
      "loss:  0.21039271523068948\n",
      "loss:  0.16888552594091777\n",
      "iteration  3\n",
      "loss:  0.17561948053980647\n",
      "loss:  0.13504340626933053\n",
      "iteration  4\n",
      "loss:  0.1426363771247055\n",
      "loss:  0.10867983033999122\n",
      "iteration  5\n",
      "loss:  0.11502893698405223\n",
      "loss:  0.08768395998855236\n",
      "iteration  6\n",
      "loss:  0.0928020762967554\n",
      "loss:  0.07077224631451146\n",
      "iteration  7\n",
      "loss:  0.07644241281989249\n",
      "loss:  0.05664430659026215\n",
      "iteration  8\n",
      "loss:  0.0621988347924094\n",
      "loss:  0.04557337804621439\n",
      "iteration  9\n",
      "loss:  0.051754025100530135\n",
      "loss:  0.03600267829596182\n",
      "iteration  10\n",
      "loss:  0.04195738853506266\n",
      "loss:  0.029016145277777173\n",
      "iteration  11\n",
      "loss:  0.03390603582194378\n",
      "loss:  0.023456035984782735\n",
      "iteration  12\n",
      "loss:  0.027406072578026783\n",
      "loss:  0.01896573894358258\n",
      "iteration  13\n",
      "loss:  0.022157030228411485\n",
      "loss:  0.015338254564410155\n",
      "iteration  14\n",
      "loss:  0.018027153022229552\n",
      "loss:  0.012341739841076255\n",
      "iteration  15\n",
      "loss:  0.015065228426284437\n",
      "loss:  0.009586368750846657\n",
      "iteration  16\n",
      "loss:  0.012188035601801967\n",
      "loss:  0.007754899098753364\n",
      "iteration  17\n",
      "loss:  0.009861272267836679\n",
      "loss:  0.006274412698935299\n",
      "iteration  18\n",
      "loss:  0.007979584992822397\n",
      "loss:  0.005077204227750456\n",
      "iteration  19\n",
      "loss:  0.006457601583694984\n",
      "loss:  0.004108905645530314\n",
      "iteration  20\n",
      "loss:  0.005226388402644042\n",
      "loss:  0.0033256274334449265\n",
      "iteration  21\n",
      "loss:  0.004321800898415697\n",
      "loss:  0.0026031644528579224\n",
      "iteration  22\n",
      "loss:  0.0035844938215735802\n",
      "loss:  0.0020081805988030844\n",
      "iteration  23\n",
      "loss:  0.002901698841104711\n",
      "loss:  0.001625671629448253\n",
      "iteration  24\n",
      "loss:  0.00234908843090879\n",
      "loss:  0.0013161533674801431\n",
      "iteration  25\n",
      "loss:  0.001901826511427837\n",
      "loss:  0.0010656452661766869\n",
      "iteration  26\n",
      "loss:  0.00153980158665896\n",
      "loss:  0.0008628781429419072\n",
      "iteration  27\n",
      "loss:  0.0012467490770640976\n",
      "loss:  0.0006987397781190795\n",
      "iteration  28\n",
      "loss:  0.0010095134715371733\n",
      "loss:  0.0005658607532270125\n",
      "iteration  29\n",
      "loss:  0.0008174525259980593\n",
      "iteration  30\n",
      "loss:  0.0008174525259980593\n",
      "iteration  31\n",
      "loss:  0.0008174525259980593\n",
      "iteration  32\n",
      "loss:  0.0008174525259980593\n",
      "iteration  33\n",
      "loss:  0.0008174525259980593\n",
      "iteration  34\n",
      "loss:  0.0008174525259980593\n",
      "iteration  35\n",
      "loss:  0.0008174525259980593\n",
      "iteration  36\n",
      "loss:  0.0008174525259980593\n",
      "iteration  37\n",
      "loss:  0.0008174525259980593\n",
      "iteration  38\n",
      "loss:  0.0008174525259980593\n",
      "iteration  39\n",
      "loss:  0.0008174525259980593\n",
      "start_i: 260/276\n",
      "start:\n",
      "[-1.10505773 -2.19351531 -4.41083422  4.77577156]\n",
      "goal:\n",
      "[-1.8320877  -1.23168706 -2.80687366  4.75535831]\n",
      "iteration  0\n",
      "loss:  0.35915517393818924\n",
      "loss:  0.358737986720097\n",
      "iteration  1\n",
      "loss:  0.358737986720097\n",
      "loss:  0.287660811512063\n",
      "iteration  2\n",
      "loss:  0.289701127494465\n",
      "loss:  0.23047402918126333\n",
      "iteration  3\n",
      "loss:  0.23612105662930943\n",
      "loss:  0.18449721739288918\n",
      "iteration  4\n",
      "loss:  0.18934517090134112\n",
      "loss:  0.14815057145670402\n",
      "iteration  5\n",
      "loss:  0.15198217495844238\n",
      "loss:  0.11907511306984545\n",
      "iteration  6\n",
      "loss:  0.1251477461910424\n",
      "loss:  0.09517899004379993\n",
      "iteration  7\n",
      "loss:  0.10083174059432962\n",
      "loss:  0.076594438147558\n",
      "iteration  8\n",
      "loss:  0.08156842066114556\n",
      "loss:  0.06160903890644794\n",
      "iteration  9\n",
      "loss:  0.06724297734644265\n",
      "loss:  0.04909939227041811\n",
      "iteration  10\n",
      "loss:  0.05418288293832181\n",
      "loss:  0.039597161881497735\n",
      "iteration  11\n",
      "loss:  0.04367961720769106\n",
      "loss:  0.03194945234503886\n",
      "iteration  12\n",
      "loss:  0.03522970800434321\n",
      "loss:  0.02578907462176247\n",
      "iteration  13\n",
      "loss:  0.028426955551527067\n",
      "loss:  0.020823946137020825\n",
      "iteration  14\n",
      "loss:  0.02294682968997363\n",
      "loss:  0.016820124102079177\n",
      "iteration  15\n",
      "loss:  0.018529692794667964\n",
      "loss:  0.013590015252599794\n",
      "iteration  16\n",
      "loss:  0.015144651121964085\n",
      "loss:  0.01090766188107626\n",
      "iteration  17\n",
      "loss:  0.012462581921687595\n",
      "loss:  0.008696435737513294\n",
      "iteration  18\n",
      "loss:  0.010254387113189498\n",
      "loss:  0.006914558713963164\n",
      "iteration  19\n",
      "loss:  0.008492878369304699\n",
      "loss:  0.0054293123225605654\n",
      "iteration  20\n",
      "loss:  0.006866721191699512\n",
      "loss:  0.004391732366726509\n",
      "iteration  21\n",
      "loss:  0.005552818026060085\n",
      "loss:  0.003553068833281153\n",
      "iteration  22\n",
      "loss:  0.004491094884839821\n",
      "loss:  0.0028749209981308054\n",
      "iteration  23\n",
      "loss:  0.0036329427850847265\n",
      "loss:  0.002326471906313461\n",
      "iteration  24\n",
      "loss:  0.0029391790085688714\n",
      "loss:  0.0018828470743077323\n",
      "iteration  25\n",
      "loss:  0.0024034930782614082\n",
      "loss:  0.0015022044088712435\n",
      "iteration  26\n",
      "loss:  0.002031212746125443\n",
      "loss:  0.001111203868808712\n",
      "iteration  27\n",
      "loss:  0.001644364992027723\n",
      "loss:  0.0008993001818773564\n",
      "iteration  28\n",
      "loss:  0.0013312867488212692\n",
      "loss:  0.0007278973738947608\n",
      "iteration  29\n",
      "loss:  0.0010779123500879286\n",
      "loss:  0.0005892117700056521\n",
      "iteration  30\n",
      "loss:  0.0008728386067686663\n",
      "iteration  31\n",
      "loss:  0.0008728386067686663\n",
      "iteration  32\n",
      "loss:  0.0008728386067686663\n",
      "iteration  33\n",
      "loss:  0.0008728386067686663\n",
      "iteration  34\n",
      "loss:  0.0008728386067686663\n",
      "iteration  35\n",
      "loss:  0.0008728386067686663\n",
      "iteration  36\n",
      "loss:  0.0008728386067686663\n",
      "iteration  37\n",
      "loss:  0.0008728386067686663\n",
      "iteration  38\n",
      "loss:  0.0008728386067686663\n",
      "iteration  39\n",
      "loss:  0.0008728386067686663\n",
      "start_i: 270/276\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for envi in range(len(paths)):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    #ax.set_autoscale_on(True)\n",
    "    ax.set_xlim(-np.pi, np.pi)\n",
    "    ax.set_ylim(-np.pi, np.pi)\n",
    "    new_obs_i = []\n",
    "    obs_i = obs[envi]\n",
    "    for k in range(len(obs_i)):\n",
    "        obs_pt = []\n",
    "        obs_pt.append(obs_i[k][0]-obs_width/2)\n",
    "        obs_pt.append(obs_i[k][1]-obs_width/2)\n",
    "        obs_pt.append(obs_i[k][0]-obs_width/2)\n",
    "        obs_pt.append(obs_i[k][1]+obs_width/2)\n",
    "        obs_pt.append(obs_i[k][0]+obs_width/2)\n",
    "        obs_pt.append(obs_i[k][1]+obs_width/2)\n",
    "        obs_pt.append(obs_i[k][0]+obs_width/2)\n",
    "        obs_pt.append(obs_i[k][1]-obs_width/2)\n",
    "        new_obs_i.append(obs_pt)\n",
    "    obs_i = new_obs_i\n",
    "\n",
    "    dtheta = 0.1\n",
    "    feasible_points = []\n",
    "    infeasible_points = []\n",
    "    imin = 0\n",
    "    imax = int(2*np.pi/dtheta)\n",
    "\n",
    "\n",
    "    for i in range(imin, imax):\n",
    "        for j in range(imin, imax):\n",
    "            x = np.array([dtheta*i-np.pi, dtheta*j-np.pi, 0., 0.])\n",
    "            if IsInCollision(x, obs_i):\n",
    "                infeasible_points.append(x)\n",
    "            else:\n",
    "                feasible_points.append(x)\n",
    "    feasible_points = np.array(feasible_points)\n",
    "    infeasible_points = np.array(infeasible_points)\n",
    "    print('feasible points')\n",
    "    print(feasible_points)\n",
    "    print('infeasible points')\n",
    "    print(infeasible_points)\n",
    "    ax.scatter(feasible_points[:,0], feasible_points[:,1], c='yellow')\n",
    "    ax.scatter(infeasible_points[:,0], infeasible_points[:,1], c='pink')\n",
    "    #for i in range(len(data)):\n",
    "    #    update_line(hl, ax, data[i])\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    #print(obs_i)\n",
    "    for pathi in range(len(paths[envi])):\n",
    "        start_state = sgs[envi][pathi][0]\n",
    "        goal_inform_state = paths[envi][pathi][-1]\n",
    "        goal_state = sgs[envi][pathi][1]\n",
    "        #p = Process(target=plan_one_path, args=(obs[i], obc[i], start_state, goal_state, 500, queue))\n",
    "\n",
    "        # propagate data\n",
    "        p_start = paths[envi][pathi][0]\n",
    "        detail_paths = [p_start]\n",
    "        detail_controls = []\n",
    "        detail_costs = []\n",
    "        state = [p_start]\n",
    "        control = []\n",
    "        cost = []\n",
    "        for k in range(len(controls[envi][pathi])):\n",
    "            #state_i.append(len(detail_paths)-1)\n",
    "            max_steps = int(costs[envi][pathi][k]/step_sz)\n",
    "            accum_cost = 0.\n",
    "            #print('p_start:')\n",
    "            #print(p_start)\n",
    "            #print('data:')\n",
    "            #print(paths[i][j][k])\n",
    "            # modify it because of small difference between data and actual propagation\n",
    "            p_start = paths[envi][pathi][k]\n",
    "            state[-1] = paths[envi][pathi][k]\n",
    "            for step in range(1,max_steps+1):\n",
    "                p_start = propagate_dynamics(p_start, controls[envi][pathi][k], step_sz)\n",
    "                p_start = enforce_state_bounds(p_start)\n",
    "                detail_paths.append(p_start)\n",
    "                detail_controls.append(controls[envi][pathi])\n",
    "                detail_costs.append(step_sz)\n",
    "                accum_cost += step_sz\n",
    "                if (step % 1 == 0) or (step == max_steps):\n",
    "                    state.append(p_start)\n",
    "                    #print('control')\n",
    "                    #print(controls[i][j])\n",
    "                    control.append(controls[envi][pathi][k])\n",
    "                    cost.append(accum_cost)\n",
    "                    accum_cost = 0.\n",
    "        #print('p_start:')\n",
    "        #print(p_start)\n",
    "        #print('data:')\n",
    "        #print(paths[i][j][-1])\n",
    "        #state[-1] = paths[i][j][-1]\n",
    "        data = state\n",
    "        \n",
    "        data = np.array(data)\n",
    "        ax.scatter(data[:,0], data[:,1], c='green', s=10)\n",
    "        ax.scatter(data[-1,0], data[-1,1], c='red', s=10, marker='*')\n",
    "        \n",
    "        \n",
    "        #for start_i in range(0, len(data), num_steps):\n",
    "        for start_i in range(0, len(data), num_steps):\n",
    "            print(\"start_i: %d/%d\"  % (start_i, len(data)))\n",
    "            #xs, us = bvp_solve(start_state, goal_state, dt, num_steps)\n",
    "            if start_i + num_steps >= len(data):\n",
    "                break\n",
    "                \n",
    "            # initialize before solving\n",
    "            x_init = np.linspace(data[start_i], data[start_i+num_steps], num_steps+1)\n",
    "            \n",
    "            std = 0.2\n",
    "            cov = np.diag([std*bound[0], std*bound[1], std*bound[2], std*bound[3]])\n",
    "            #mean = next_state\n",
    "            #next_state = np.random.multivariate_normal(mean=mean,cov=cov)\n",
    "            mean = np.zeros(data[start_i].shape)\n",
    "            rand_x_init = np.random.multivariate_normal(mean=mean, cov=cov, size=num_steps+1)\n",
    "            \n",
    "            x_init = rand_x_init + x_init\n",
    "            x_init[0] = data[start_i]\n",
    "            x_init[-1] = data[start_i+num_steps]\n",
    "            print('start:')\n",
    "            print(data[start_i])\n",
    "            print('goal:')\n",
    "            print(data[start_i+num_steps])\n",
    "            #print('x_init:')\n",
    "            #print(x_init)\n",
    "            u_init = np.random.uniform(-4., 4., size=(num_steps, 1))\n",
    "            #print('intial xs:')\n",
    "            #print(x_init)\n",
    "            #print('initial us:')\n",
    "            #print(u_init)\n",
    "            # propagate bvp result\n",
    "            p_start = x_init[0]\n",
    "            detail_paths = [p_start]\n",
    "            detail_controls = []\n",
    "            detail_costs = []\n",
    "            state = [p_start]\n",
    "            control = []\n",
    "            for k in range(len(u_init)):\n",
    "                #state_i.append(len(detail_paths)-1)\n",
    "                # modify it because of small difference between data and actual propagation\n",
    "                p_start = propagate_dynamics(p_start, u_init[k], step_sz)\n",
    "                p_start = enforce_state_bounds(p_start)\n",
    "                detail_paths.append(p_start)\n",
    "                detail_controls.append(u_init[k])\n",
    "                state.append(p_start)\n",
    "                #print('control')\n",
    "                #print(controls[i][j])\n",
    "            state = np.array(state)\n",
    "            ax.scatter(state[:,0], state[:,1], c='brown', s=30)\n",
    "\n",
    "            \n",
    "            #xs, us = bvp_solve(x_init, u_init, step_sz, num_steps)\n",
    "            xs, us = bvp_solve_naive(x_init, u_init, step_sz, num_steps)\n",
    "            #plan_one_path(obs_i, obs[i], obc[i], start_state, goal_state, goal_inform_state, 1000, data)\n",
    "            #print('optimized xs:')\n",
    "            #print(xs)\n",
    "            #print('optimzied us:')\n",
    "            #print(us)\n",
    "            ax.scatter(xs[:,0], xs[:,1], c='lightgreen', s=10)\n",
    "            \n",
    "            # check the optimization result loss\n",
    "            \"\"\"\n",
    "            print('propagation to see the loss:')\n",
    "            for i in range(len(xs)-1):\n",
    "                x_new = propagate_dynamics(xs[i], us[i], step_sz)\n",
    "                print('from: ')\n",
    "                print(xs[i])\n",
    "                print('to: ')\n",
    "                print(x_new)\n",
    "                print('predicted: ')\n",
    "                print(xs[i+1])\n",
    "                print(np.sum((x_new - xs[i+1]) ** 2))\n",
    "            \"\"\"\n",
    "            \n",
    "            \n",
    "            # propagate bvp result\n",
    "            p_start = xs[0]\n",
    "            detail_paths = [p_start]\n",
    "            detail_controls = []\n",
    "            detail_costs = []\n",
    "            state = [p_start]\n",
    "            control = []\n",
    "            for k in range(len(us)):\n",
    "                #state_i.append(len(detail_paths)-1)\n",
    "                # modify it because of small difference between data and actual propagation\n",
    "                p_start = propagate_dynamics(p_start, us[k], step_sz)\n",
    "                p_start = enforce_state_bounds(p_start)\n",
    "                detail_paths.append(p_start)\n",
    "                detail_controls.append(us[k])\n",
    "                state.append(p_start)\n",
    "                #print('control')\n",
    "                #print(controls[i][j])\n",
    "            state = np.array(state)\n",
    "            ax.scatter(state[:,0], state[:,1], c='blue', s=10)\n",
    "        plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ylmiao_kino]",
   "language": "python",
   "name": "conda-env-ylmiao_kino-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
